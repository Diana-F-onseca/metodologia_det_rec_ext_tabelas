# -*- coding: utf-8 -*-
"""metodologia_ocr_aplicado_1_vez.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Okfb1dTtlOWGIY4-w2YEOwRS22kSC8Q4
"""

# Importação das bibliotecas
import cv2
from PIL import Image, ImageOps, ImageDraw, ImageFont
import numpy as np
from ultralytics import YOLO
from pathlib import Path
import io
import requests
import time
import numpy as np
import tensorflow as tf
import os
import math
from statistics import mode, StatisticsError

import json

from modules_1_image.divide_info_1 import *
from modules_1_image.ocr_manipulation import *
from modules_1_image.text_list_manipulation_1 import *
from modules_1_image.coor_manipulation_1 import *
from modules_1_image.excel_json_manipulation_1 import *


# Definição da classe AzureOCRApi (fornecida)

class AzureOCRApi():
    def __init__(self, endpoint, subscription_key):
        self.endpoint = endpoint
        self.subscription_key = subscription_key

    def _post_analyze(self, image_data):
        ocr_url = f"{self.endpoint}/read/analyze"
        headers = {'Ocp-Apim-Subscription-Key': self.subscription_key,
                   'Content-Type': 'application/octet-stream'}
        params = {'detectOrientation': 'true'}
        response = requests.post(ocr_url, headers=headers, params=params, data=image_data)
        response.raise_for_status()
        return response

    def _read_analyze(self, response):
        read_operation_location = response.headers["Operation-Location"]
        operation_id = read_operation_location.split("/")[-1]
        ocr_url = f"{self.endpoint}/read/analyzeResults/{operation_id}"
        headers = {'Ocp-Apim-Subscription-Key': self.subscription_key, 'Accept': 'application/json'}
        response = requests.get(ocr_url, headers=headers)
        response.raise_for_status()
        return response.json()

    def _read_results(self, results: dict):
        text_total = []
        angle = results['analyzeResult']['readResults'][0]['angle']

        # Iterar por cada resultado da leitura
        for text_result in results['analyzeResult']['readResults']:
            for line in text_result['lines']:
                # A caixa delimitadora vem no formato de lista [x1, y1, x2, y2, x3, y3, x4, y4]
                boundingBox = line['boundingBox']
                text_total.append({
                    'text': line['text'],
                    'boundingBox': boundingBox,  # Adiciona a caixa delimitadora ao resultado
                    'conf': line.get('appearance', {}).get('style', {}).get('confidence', 1)
                })
        return text_total, angle


    def get_ocr_azure(self, file):
        response = self._post_analyze(image_data=file)
        while True:
            results = self._read_analyze(response=response)
            if results['status'] not in ['notStarted', 'running']:
                break
            time.sleep(0.5)
        text_result, angle = self._read_results(results)
        return text_result, angle

    # acrescentado
    def annotate_image(self, image, text_result, output_path):
        draw = ImageDraw.Draw(image)
        font = ImageFont.load_default()

        for item in text_result:
            text = item['text']
            bbox = item['boundingBox']

            # Desenhar a caixa delimitadora
            draw.line([(bbox[i], bbox[i+1]) for i in range(0, len(bbox), 2)] + [(bbox[0], bbox[1])],
                      fill="red", width=2)

            # Calcular a posição para escrever o texto
            text_position = (bbox[0], bbox[1] - 10)  # Texto acima da caixa que o delimita

            # Escrever o texto
            draw.text(text_position, text, fill="red", font=font)

        # Guardar a imagem anotada
        image.save(output_path)
        print(f"Imagem anotada guardada em: {output_path}")

# Método que converte a imagem em formato de bytes
def image_to_bytes(image_cv):
    # Usar o imencode para converter a imagem OpenCV num formato de memória (array de bytes)
    _, buffer = cv2.imencode('.jpg', image_cv)  # Codifica a imagem como JPEG
    io_buf = io.BytesIO(buffer)  # Cria um buffer de bytes
    return io_buf.getvalue()  # Retorna os bytes da imagem


# Redimensiona a imagem, com respeito às proporções originais, para as dimensões necessárias
def load_and_resize_image(image_path, size):
    """
    Carrega e redimensiona a imagem para 2400x2400, com o PIL.
    """
    image = Image.open(image_path)
    image_resized = ImageOps.pad(image, (size, size), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')
    return image_resized, image_resized.width, image_resized.height

# Rotação da imagem, com o valor de ângulo retornado pelo OCR Azure
# A imagem colocada como neste método tem de estar em formato PIL.
def rotate_image(image_file, angle):
    """
    Aplica a rotação numa imagem com o valor de ângulo passado como parâmetro nesta função.

    image consiste na imagem de entrada no formato de array numpy (lida com cv2.imread).
    angle, consiste no valor do ângulo em graus, com o qual a imagem será rodada.

    Retorna a Imagem rodada no formato de array do numpy.
    """

    rotated_image = image_file.rotate(angle, expand=False)

    return rotated_image, rotated_image.width, rotated_image.height


# Previsão da bounding box de maior score na imagem, que retorna a bounding box redimensionada para uma imagem equivalente de dimensões 2400 por 2400 prevista e o valor de Score da previsão
def predict_bboxes(image, model):
    """
    Faz a previsão das caixas delimitadoras com o modelo YOLO treinado.
    """
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    predictions = model(image_cv, imgsz=640)
    bboxes = []
    if hasattr(predictions[0].boxes, 'xyxy') and hasattr(predictions[0].boxes, 'conf'):
        for i, box in enumerate(predictions[0].boxes.xyxy):
            bboxes.append((box.tolist(), predictions[0].boxes.conf[i].item()))
    # print(bboxes)
    if not bboxes:
        # print(f"Sem Previsão de Tabela")
        return
    else:
        sorted_pred = sorted(bboxes, key=lambda x: x[1])
        # box_pred é a bounding box com maior valor de score na previsão
        box_pred = list(sorted_pred[0][0])
        # print(type(box_pred))
        box_pred_2400 = [box_pred[0]*(2400/640),box_pred[1]*(2400/640),box_pred[2]*(2400/640),box_pred[3]*(2400/640)]
        # print('Coordenadas da bounding box prevista', box_pred_2400)
        score_pred = np.round(float(sorted_pred[0][1]),3)
        # print('Score da bounding box prevista:', score_pred)
        return box_pred_2400, score_pred # pred[0][0] x_cse e pred[0][1] y_cse

def save_annotated_image(image, bboxes_scores, nome_ficheiro):

    # Este método Desenha as caixas delimitadoras e imprime os scores numa imagem PIL e guarda a imagem anotada.
    # Recebe uma imagem no formato PIL, uma bboxes_scores, que consiste numa lista de tuplos com caixas delimitadoras e os scores associados
    # e o caminho original da imagem na extração do nome do ficheiro

    # Converte a imagem PIL para o formato OpenCV
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    for bbox, score in bboxes_scores:
        x1, y1, x2, y2 = bbox
        cv2.rectangle(image_cv, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Borda vermelha
        cv2.putText(image_cv, f'Score: {score:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    string = 'output_data/tableDetection/' + nome_ficheiro + '.jpg'
    # Guarda a imagem anotada
    cv2.imwrite(os.path.join(string), image_cv)

# Método que guarda (e mostra) a imagem recortada
def crop(image, bbox):
    """
    Recorta a imagem para a bounding box com maior score.
    """
    if not bbox:
        print("Nenhuma bounding box encontrada.")
        return None, "No bounding boxes found", None

    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    x1, y1, x2, y2 = map(int, bbox)
    cropped_image_cv = image_cv[y1:y2, x1:x2]

    # Converte a imagem para tipo PIL para que possa ser guardada
    cropped_image_pil = Image.fromarray(cv2.cvtColor(cropped_image_cv, cv2.COLOR_BGR2RGB))

    return cropped_image_pil


# criar uma extensão da classe com a chave e url fornecidos
azure_api = AzureOCRApi('https://westeurope.api.cognitive.microsoft.com/vision/v3.2', 'b700dee2252d45e589852898cc6b56a0')

# 2400x2400
# OCR - angulo e texto
# Rodar e caso ter de rodar a imagem, ajustar as coordenadas do OCR
# 640x640
# chamar previsao das tabelas
# região de interesse - coordenadas
# converter para 2400 por 2400 (coordenadas)
# Filtro do OCR para as coordenadas da tabela
# Rotação das coordenadas do OCR

# método que roda um ponto em torno de outro ponto (cx, cy) com um valor de ângulo passado como parâmetro
def rotate_point(cx, cy, angle, px, py):
    # conversão dos ângulos em radianos, porque as funções trigonométricas recebem radianos
    seno = math.sin(math.radians(angle))
    cosseno = math.cos(math.radians(angle))

    # Translada o ponto de volta à origem:
    # Subtrai as coordenadas do ponto central (cx, cy) das coordenadas do ponto (px, py). Faz com que o centro (cx, cy) corresponda à origem (0,0).
    px -= cx
    py -= cy

    # Roda o ponto
    # Calcula as novas coordenadas do ponto rodado, com as fórmulas de rotação:
    xrot = px * cosseno - py * seno
    yrot = px * seno + py * cosseno

    # Translada o ponto tendo em conta as coordenadas de centro originais:
    px = xrot + cx
    py = yrot + cy
    return int(px), int(py)

# converter os bytes da imagem para um objeto do tipo PIL.Image
def bytes_to_pil(image_bytes):
    image_ = io.BytesIO(image_bytes)
    image = Image.open(image_)
    return image

# Rodar as caixas delimitadoras
def rotate_bounding_boxes(imagem, anotacoes, angle):
    # o ângulo chamado é o simétrico do ângulo da rotação que as caixas delimitadoras vão sofrer
    if isinstance(imagem, bytes):
       imagem = bytes_to_pil(imagem)
    # Centro da imagem para rotação
    height, width = imagem.height, imagem.width

    cx, cy = width // 2, height // 2

    # Novas anotações após a rotação
    new_annotations = []

    for annotation in anotacoes:
        # Obter os pontos da caixa delimitadora atual
        points = annotation['boundingBox']
        rotated_points = []

        # Pontos no formato [x1, y1, x2, y2, x3, y3, x4, y4]
        # o ciclo é percorrido de 2 em 2
        for i in range(0, len(points), 2):
            px, py = points[i], points[i+1]
            rotated_px, rotated_py = rotate_point(cx, cy, angle, px, py)
            rotated_points.extend([rotated_px, rotated_py])

        # Guardar a nova caixa delimitadora rodada
        new_annotations.append({
            'text': annotation['text'],
            'boundingBox': rotated_points,
            'conf': annotation['conf']
        })

    return new_annotations

# converte o formato de 4 cantos para o formato de canto superior esquerdo e canto superior direito
def convert_bounding_boxes(data):
    text_ocr_to_images = []
    for item in data:
        text = item['text']
        conf = item['conf']
        # [x1, y1, x2, y2, x3, y3, x4, y4]
        bounding_box = item['boundingBox']
        # Calcula os valores mínimos e máximos de x e y
        x_min = min(bounding_box[0], bounding_box[2], bounding_box[4], bounding_box[6])
        y_min = min(bounding_box[1], bounding_box[3], bounding_box[5], bounding_box[7])
        x_max = max(bounding_box[0], bounding_box[2], bounding_box[4], bounding_box[6])
        y_max = max(bounding_box[1], bounding_box[3], bounding_box[5], bounding_box[7])
        # Cria uma nova caixa delimitadora
        new_bounding_box = [x_min, y_min, x_max, y_max]
        # Adiciona o novo item do dicionário
        text_ocr_to_images.append({
            'text': text,
            'boundingBox': new_bounding_box,
            'conf': conf
        })
    return text_ocr_to_images

def bytes_to_cv2(image_bytes):
    # Converter bytes para um array numpy
    numpy_array = np.frombuffer(image_bytes, np.uint8)
    # Ler a imagem como BGR (padrão do OpenCV)
    cv2_image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
    return cv2_image

def pil_to_cv2(pil_image):
    # Converter PIL Image para um array NumPy
    numpy_image = np.array(pil_image)
    # Converter de RGB para BGR
    cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)
    return cv2_image

def annotate_image_with_inclined_bounding_boxes(image, annotations):
    if isinstance(image, bytes):
        # Converter de bytes para OpenCV
        image = bytes_to_cv2(image)
        print("Imagem processada como bytes.")
    elif isinstance(image, Image.Image):
        # Converta de PIL para OpenCV
        image = pil_to_cv2(image)
        print("Imagem processada como PIL Image.")
    if image is None:
        print("Erro ao carregar a imagem.")
        return

    # Configurações para o texto
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (255, 255, 255)  # Branco
    font_thickness = 1
    box_color = (0, 0, 255)  # Verde
    box_thickness = 2

    for annotation in annotations:
        # Caixa delimitadora com oito valores x, y para os quatro pontos
        pts = np.array([
            annotation['boundingBox'][0:2],
            annotation['boundingBox'][2:4],
            annotation['boundingBox'][4:6],
            annotation['boundingBox'][6:8]
        ], np.int32)
        pts = pts.reshape((-1, 1, 2))

        # Desenhar a caixa delimitadora inclinada
        cv2.polylines(image, [pts], isClosed=True, color=box_color, thickness=box_thickness)

        # Calcular a posição do texto (na média das coordenadas x e y dos pontos)
        text_x = int(np.mean(pts[:, 0, 0]))
        text_y = int(np.mean(pts[:, 0, 1]))

        # Desenhar o texto
        cv2.putText(image, annotation['text'], (text_x, text_y), font, font_scale, font_color, font_thickness)

    # Guardar a imagem anotada
    cv2.imwrite('output_data/tableStructure/tabela_anotada_com_as_bboxes_rodadas.jpg', image)
    print("Imagem anotada guardada como 'annotated_rotated_image.jpg'.")

# Método que remove todas as deteções do OCR que se encontram fora da região da tabela prevista
# Filtra e retorna apenas as caixas delimitadoras do OCR que estão completamente dentro da caixa delimitadora prevista
# O método recebe uma lista ocr_bboxes no formato {'text': str, 'boundingBox': list, 'conf': float} e uma lista de caixas delimitadoras previstas no formato [x_min, y-min, x_max, y_max]
def filter_bboxes_inside_predicted_region(ocr_bboxes, predicted_bbox):
    filtered_bboxes = []
    for bbox in ocr_bboxes:
        points = [
            (bbox['boundingBox'][0], bbox['boundingBox'][1]),
            (bbox['boundingBox'][2], bbox['boundingBox'][3]),
            (bbox['boundingBox'][4], bbox['boundingBox'][5]),
            (bbox['boundingBox'][6], bbox['boundingBox'][7])
        ]

        # Condição de todos os pontos da bounding box do OCR estarem dentro da região prevista
        if all(predicted_bbox[0] <= x <= predicted_bbox[2] and predicted_bbox[1] <= y <= predicted_bbox[3] for x, y in points):
            filtered_bboxes.append(bbox)
    # Retornar todas as deteções de OCR no interior da bounding box prevista
    return filtered_bboxes

# Ajustar as coordenadas das caixas delimitadoras com a subtração do canto superior esquerdo da previsão
# O método recebe os parâmetros only_inside_ocr_bboxes, que é uma lista de dicionários com as caixas delimitadoras de texto do OCR e um parâmetro pred que recebe as caixas delimitadoras previstas
# Retorna uma lista de dicionários atualizada com as caixas delimitadoras ajustadas.
def adjust_bboxes_to_pred(only_inside_ocr_bboxes, pred):
    x1_pred, y1_pred = pred[0], pred[1]
    print('y1_pred', y1_pred)
    adjusted_bboxes = []
    for item in only_inside_ocr_bboxes:
        # Coordenadas da bounding box original
        original_bbox = item['boundingBox']
        # Ajusta cada ponto da bounding box
        adjusted_bbox = [
            original_bbox[0] - x1_pred, original_bbox[1] - y1_pred,
            original_bbox[2] - x1_pred, original_bbox[3] - y1_pred,
            original_bbox[4] - x1_pred, original_bbox[5] - y1_pred,
            original_bbox[6] - x1_pred, original_bbox[7] - y1_pred,
        ]
        # Atualiza o dicionário com a bounding box com os valores ajustados para as bounding boxes serem correspondentes com as da imagem recortada
        adjusted_item = item.copy()
        adjusted_item['boundingBox'] = adjusted_bbox
        adjusted_bboxes.append(adjusted_item)

    return adjusted_bboxes

# converte os formatos das caixas delimitadoras para o formato retangular [x1,y1,x2,y2]
# recebe adjusted_bboxes que é uma lista de caixas delimitadoras no formato poligonal
def convert_to_rectangular_format(adjusted_bboxes):
    rectangular_bboxes = []
    for item in adjusted_bboxes:
        # Definem-se os pontos com as coordenadas ajustadas à imagem recortada
        points = item['boundingBox']
        x_coords = points[0::2]  # Todos os valores de x
        y_coords = points[1::2]  # Todos os valores de y

        # Encontra os valores mínimo e máximo para criar o retângulo
        x1 = min(x_coords) # coordenada x do canto superior esquerdo
        y1 = min(y_coords) # coordenada y do canto superior esquerdo
        x2 = max(x_coords) # coordenada x do canto inferior direito
        y2 = max(y_coords) # coordenada y do canto inferior direito

        # Formato retangular da caixa delimitadora que deu entrada, como parâmetro
        rectangular_bbox = [x1, y1, x2, y2]

        # Atualiza o dicionário das deteções de OCR com a caixa delimitadora convertida para o formato retangular
        rectangular_item = item.copy()
        rectangular_item['boundingBox'] = rectangular_bbox
        rectangular_bboxes.append(rectangular_item)

    return rectangular_bboxes

# método que converte uma imagem PIL, de entrada, em bytes
def pil_image_to_bytes(image_pil):
        # Cria um buffer de bytes
        io_buf = io.BytesIO()

        # Guarda a imagem em PIL no buffer de bytes
        image_pil.save(io_buf, format='JPEG')

        # Retorna os bytes do buffer
        return io_buf.getvalue()

# Função que realiza a previsão das caixas delimitadoras numa imagem redimensionada com o modelo YOLO treinado, que dá entrada no método, como parâmetro.
# O método recebe uma imagem em formato PIL e o modelo treinado
def predict_bboxes_c_l(image, modelo):
    """
    Função para prever as bounding boxes numa imagem redimensionada com o modelo YOLO pré-treinado.

    Parâmetros:
    image: Imagem PIL redimensionada na qual se vão prever as bounding boxes.

    Retorna:
    Uma lista de tuplos, onde cada tuplo contém as coordenadas da bounding box e o score da previsão.
    """
    # Converte a imagem PIL para o formato OpenCV
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    # Faz a previsão com o modelo YOLO
    predictions = modelo(image_cv)
    bboxes = []
    if hasattr(predictions[0].boxes, 'xyxy') and hasattr(predictions[0].boxes, 'conf'):
        xyxy = predictions[0].boxes.xyxy
        confs = predictions[0].boxes.conf
        for i, box in enumerate(xyxy):
            x1, y1, x2, y2 = map(int, [box[0], box[1], box[2], box[3]])
            score = confs[i]
            bboxes.append(((x1, y1, x2, y2), score.item()))
    return bboxes

# Método que converte uma imagem em PIL para o formato cv2
def pil_to_cv2(imagem_pil):
    # Converter a imagem PIL para um numpy array
    imagem_np = np.array(imagem_pil)

    # Converter a imagem de RGB (PIL) para BGR (OpenCV)
    imagem_cv2 = cv2.cvtColor(imagem_np, cv2.COLOR_RGB2BGR)
    return imagem_cv2

def process_images_with_nms_and_vertical_lines(results, imagem, output_directory_nms):
    Path(output_directory_nms).mkdir(parents=True, exist_ok=True)
    horizontal_apos_nms = []

    if isinstance(imagem, Image.Image):
        image = pil_to_cv2(imagem)

    if image is None:
        print(f"Imagem não encontrada.")

    image_width = image.shape[1]
    image_height = image.shape[0]

    # Garantir que a box_coords é uma lista de listas (a 2 dimensões)
    box_coords = [[int(box[0][0]), 0, int(box[2][0]), image_height] for box, _ in results]

    # Verificar se box_coords é não-vazia e tem o formato correto
    if box_coords:
        box_tensor = tf.convert_to_tensor(box_coords, dtype=tf.float32)
        score_tensor = tf.convert_to_tensor([score for _, score in results], dtype=tf.float32)

        # Aplicar o método Non-Maximum Suppression
        selected_indices = tf.image.non_max_suppression(
            boxes=box_tensor,
            scores=score_tensor,
            max_output_size=1000,
            iou_threshold=0.000001,
            score_threshold=float('-inf')
        ) # os parâmetros fazem com que as caixas delimitadoras eliminadas sejam apenas as que são idênticas

        selected_boxes = tf.gather(box_coords, selected_indices).numpy()
        for x1, y1, x2, y2 in selected_boxes:
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)
        horizontal_apos_nms = list(selected_boxes)
        output_image_path = output_directory_nms + '/delimitacao_vertical_da_tabela_apos_nms.jpg'
        cv2.imwrite(str(output_image_path), image)
        print(f"Imagem processada com o método NMS e guardada na diretoria: {output_image_path}")

    return horizontal_apos_nms

def process_images_with_nms_and_horizontal_lines(results, image_cv):
    if isinstance(image_cv, Image.Image):
        image = pil_to_cv2(image_cv)
    # image = cv2.imread(str(image_path))
    if image is None:
        print(f"Imagem não encontrada.")

    horizontal_apos_nms = []

    image_width = image.shape[1]
    probabilities = [score for _, score in results]

    # Garantir que a box_coords seja uma lista de listas (a 2 Dimensões)
    box_coords = [[0, int(box[0][1]), image_width, int(box[2][1])] for box, _ in results]

    # Verifica se a box_coords é não-vazia
    if box_coords:
        box_tensor = tf.convert_to_tensor(box_coords, dtype=tf.float32)
        score_tensor = tf.convert_to_tensor(probabilities, dtype=tf.float32)

        # Aplicar o método NMS com os parâmetros que melhor se adequam para a remoção destas linhas
        selected_indices = tf.image.non_max_suppression(
            boxes=box_tensor,
            scores=score_tensor,
            max_output_size=1000,
            iou_threshold=0.000001,
            score_threshold=float('-inf')
        ) # os parametros garantem que as caixas delimitadoras eliminadas sejam apenas as identicas às que se mantêm

        horizontal_apos_nms = tf.gather(box_coords, selected_indices).numpy()
        for x1, y1, x2, y2 in horizontal_apos_nms:
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)
        horizontal_apos_nms = list(horizontal_apos_nms)
        cv2.imwrite(str('output_data/tableStructure/linhas_horizontais_com_nms.jpg'), image)
        print(f"Imagem processada com o método NMS e guardada.")

    return horizontal_apos_nms

## Método que retorna as dimensões das imagens
def get_image_dimensions(image_pil):
    return image_pil.width, image_pil.height

# método que redimensiona a imagem
def redimensionar_imagem(imagem):

    # Calcula a razão de aspeto (aspect_ratio)
    largura, altura = imagem.size
    if largura > altura:
        nova_largura = 640
        nova_altura = int((altura * 640) / largura)
    else:
        nova_altura = 640
        nova_largura = int((largura * 640) / altura)

    # Redimensionar a imagem
    imagem_redimensionada = imagem.resize((nova_largura, nova_altura), Image.Resampling.LANCZOS)

    return imagem_redimensionada

def verificar_textos_duplicados(celulas_textos):
    duplicados = {}
    for (row, col), text in celulas_textos.items():
        if row not in duplicados:
            duplicados[row] = {}
        if text not in duplicados[row]:
            duplicados[row][text] = []
        duplicados[row][text].append(col)
    return duplicados

def ajustar_textos_duplicados(celulas_textos):
    duplicados = verificar_textos_duplicados(celulas_textos)
    for row, texts in duplicados.items():
        for text, cols in texts.items():
            if len(cols) > 1:
                # Manter o texto apenas na primeira coluna onde aparece
                primeira_col = min(cols)
                for col in cols:
                    if col != primeira_col:
                        celulas_textos.pop((row, col))
    return celulas_textos

def intersecao_retangulos(rect1, rect2):
    # os rect1 e rect2 são tuplos com as coordenadas (x_min, y_min, x_max, y_max)
    x_min1, y_min1, x_max1, y_max1 = rect1
    x_min2, y_min2, x_max2, y_max2 = rect2

    # Cálculo das coordenadas da área de interseção
    x_min_inter = max(x_min1, x_min2)
    y_min_inter = max(y_min1, y_min2)
    x_max_inter = min(x_max1, x_max2)
    y_max_inter = min(y_max1, y_max2)

    # Verifica se existe interseção (se as coordenadas forem válidas)
    if x_min_inter < x_max_inter and y_min_inter < y_max_inter:
        largura_inter = x_max_inter - x_min_inter
        altura_inter = y_max_inter - y_min_inter
        area_intersecao = largura_inter * altura_inter
    else:
        area_intersecao = 0  # Não existe interseção

    return area_intersecao

# Função para contar o número de interseções
def contar_intersecoes_linhas(bbox, bboxes_list):
    intersecoes = 0
    proporcoes = []
    for bbox2 in bboxes_list:
        if intersects_json(bbox, bbox2):
            intersecoes += 1
            proporcoes.append((intersecao_retangulos(bbox, bbox2)/intersecao_retangulos(bbox,bbox)))
    return intersecoes, proporcoes


def remover_ultima_somar_penultima(lista_floats):
    if len(lista_floats) < 2:
        raise ValueError("A lista deve conter pelo menos dois elementos.")

    # Remove o último elemento da lista e armazena seu valor
    ultimo_valor = lista_floats.pop()

    # Adiciona o valor removido ao valor que agora é o último da lista
    lista_floats[-1] += ultimo_valor

    return lista_floats

# método que divide uma string que se interseta com mais do que uma caixa delimitadora de célula
# em vários troços de texto (strings), de acordo com as proporções de interseção
def dividir_string_por_proporcoes(s, proporcoes):
    palavras = s.split(' ')
    total_caracteres = len(s)
    lista_resultante = []
    if len(proporcoes)==1:
      lista_resultante.append(s)
      return lista_resultante

    i = 0
    for idx, proporcao in enumerate(proporcoes):
        # Se for a última iteração, incluir todas as palavras restantes na última string
        if idx == len(proporcoes) - 1:
            string_restante = ' '.join(palavras[i:])
            lista_resultante.append(string_restante)
            break

        limite_caracteres = total_caracteres * proporcao
        string_atual = ''
        caracteres_atual = 0

        while i < len(palavras) and (caracteres_atual + len(palavras[i]) + (1 if caracteres_atual > 0 else 0)) <= limite_caracteres:
            if string_atual:
                string_atual += ' '  # Adicionar um espaço entre as palavras
                caracteres_atual += 1  # Contar com o espaço

            string_atual += palavras[i]
            caracteres_atual += len(palavras[i])
            i += 1

        lista_resultante.append(string_atual)

    # Garantir que a lista tenha exatamente o número de strings corresppondente ao número de células com que a caixa de texto se interseta
    while len(lista_resultante) < len(proporcoes):
        lista_resultante.append('')

    if len(lista_resultante) > len(proporcoes):
        # Unir todas as strings restantes na última posição, para garantir que a lista tenha não tenha mais strings do que células a intersetar
        excess_string = ' '.join(lista_resultante[2:])
        lista_resultante = lista_resultante[:2] + [excess_string]
    props_resultantes = []
    for i in range(len(lista_resultante)):
      props_resultantes.append(len(lista_resultante[i])/total_caracteres)
    print(props_resultantes)
    return lista_resultante


def annotate_image(image_path, ocr_results, output_path):
    # Carrega a imagem
    image = cv2.imread(image_path)

    # Itera sobre cada item em ocr_results
    for result in ocr_results:
        # Extrai as coordenadas da caixa delimitadora e o texto
        points, text = result

        # Desenha o polígono que representa a caixa delimitadora
        pts = [(points[i], points[i + 1]) for i in range(0, len(points), 2)]
        pts = np.array(pts, np.int32)
        pts = pts.reshape((-1, 1, 2))
        cv2.polylines(image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)

        # Escreve o texto reconhecido na imagem
        cv2.putText(image, text, (points[0], points[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)

    # Guarda a imagem anotada
    cv2.imwrite(output_path, image)


# Função principal para processar os dados e gerar os ficheiros em formato estruturado JSON e a representação da tabela num ficheiro de Excel
def create_excel_and_json(lista_sem_linha_final_amais, ocr_data):
        print(len(lista_sem_linha_final_amais[1]))
        # para evitar que nos casos em que o OCR se interseta com mais que uma bounding box, apareça a mesma string ainda por dividir
        string_anterior = ''
        # ir buscar o nome do ficheiro sem extensão da lista dos resultados de OCR
        # lista onde se encontrarão as coordenadas das células
        bboxes_celulas_todas = []
        # lista onde estarão os ids das células
        id_celula_linha_coluna = []
        # Remoção dos valores a mais da lista com as coordenadas das células
        x_values = sorted(remover_duplicados(lista_sem_linha_final_amais[0]))
        y_values = sorted(remover_duplicados(lista_sem_linha_final_amais[1]))
        ocr_partido = False
        ocupado = False
        # criação da lista vazia, que terá os resultados finais do ficheiro JSON
        dados_formatados_para_1_fornecedor = []
        # Lista que vai conter as caixas delimitadoras das células dos cabeçalhos
        bboxes_cabecalho = []
        coluna_atual = -1
        for j in range(len(y_values)-1):
            for i in range(len(x_values)-1):
                bbox_cells = [x_values[i], y_values[j], x_values[i + 1], y_values[j + 1]]
                bboxes_celulas_todas.append(bbox_cells)
                id_celula_linha_coluna.append([j, i])
                if j == 0:
                    bboxes_cabecalho.append(bbox_cells)

        # criar e preencher a lista que vai ter todas as caixas delimitadoras e os textos detetados pelo OCR, com as coordenadas de 2 cantos
        bboxes_textos_todos = []
        for i in range(len(ocr_data)):
            bbox_fixed = convert_bbox_8_to_4(ocr_data[i][0])
            bboxes_textos_todos.append([bbox_fixed, ocr_data[i][1]])
        # criar o dicionário com as coordenadas das células e o texto que vai aparecer no interior das células
        celulas_textos = {}
        palavras_adicionadas = set()
        count_headers = 0
        for i in range(len(bboxes_celulas_todas)):
            if id_celula_linha_coluna[i][0] == 0:
                count_headers+=1
        # percorrer todas as caixas delimitadoras com as coordenadas das células das tabelas
        for i in range(len(bboxes_celulas_todas)):
            # no caso de pertencer ao cabeçalho:
            if id_celula_linha_coluna[i][0] == 0:  # Cabeçalho
                print(id_celula_linha_coluna[i])
                for j in range(len(bboxes_textos_todos)):
                    # considerar apenas as bounding boxes de OCR for uma lista ou um tuplo e se as bounding boxes de texto se intersetarem com as bboxes de células
                    bboxes_textos_todos_cabec = bboxes_textos_todos[j][0]
                    texto_bboxes_textos_todos_cabec = bboxes_textos_todos[j][1]
                    bboxes_celulas_todas_atual = bboxes_celulas_todas[i]
                    if isinstance(bboxes_textos_todos[j][0], (list, tuple)) and intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j][0]):
                        print('bounding boxes cabecalho OCR', bboxes_textos_todos[j][1])
                        # se uma caixa delimitadora de texto se intersetar com mais de uma bbox de células da estutura da tabela
                        if contar_intersecoes(bboxes_textos_todos[j][0], bboxes_cabecalho) > 1:
                            # vamos extrair o texto presente nessa caixa de texto
                            string = bboxes_textos_todos[j][1]
                            # se o conteudo da string tiver '|', a string é dividida no local onde aparecer esse caracter
                            palavras = DivideInfo().run(string)
                            # é definido em que coluna se coloca a palavra
                            col_idx = id_celula_linha_coluna[i][1]

                            # para cada troço de texto, verificar se a palavra já foi adicionada
                            iterador = 0
                            for palavra in palavras:
                                print(palavra)
                                ocupado = False
                                # se o troço ainda não tiver sido adicionado
                                if palavra not in palavras_adicionadas:
                                    bboxes_celulas_todas_coluna_anterior = bboxes_celulas_todas[i]
                                    bboxes_textos_todos_coluna_anterior = bboxes_textos_todos[j-1][0]
                                    # bounding box interseta-se com ocr anterior
                                    interseta_se_com_ocr_anterior = intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0])
                                    # garantir que a proxima coluna não fica vazia é importante no ilovepdf
                                    bbox_seguinte_intersetase_com_cabecalho_seguinte = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0])
                                    count_headers
                                    bbox_seguinte_interseta_se_com_ocr_atual = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0])
                                    # interseta-se com uma coluna anterior?
                                    if intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]) or intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0]):
                                        print('interseta-se com a coluna com id', col_idx)
                                        # A palavra a adicionar vai ser a ultima palavra adicionada e depois junta-se-lhe o resto, porque algumas estão recortadas e não poderia ser simplesmente o OCR anterior, como no caso da imagem ISB_page_4.
                                        if ocr_partido:
                                            word_to_add = celulas_textos[(0, col_idx)]+' '+palavra
                                        elif not intersects(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]):
                                            col_idx += 1
                                            celulas_textos[(0, col_idx)]=palavra
                                            ocupado = True
                                            continue

                                        else:
                                            word_to_add=bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        ocupado = True

                                    # quero o caso em que a bbox de texto se interseta com a celula anterior e com a seguinte
                                    if coluna_atual == -1:
                                        cabec_anterior_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j][0])
                                    elif coluna_atual != -1:
                                        # NB PIECES tem de entrar
                                        # o cabeçalho atual interseta-se com o OCR atual
                                        cabec_anterior_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[coluna_atual], bboxes_textos_todos[j][0]) # não sei se não deveria retirar daqui o -2
                                    cabec_seguinte_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0])
                                    # interferesa_2_page_2 tem de entrar aqui, mas o interferesa_1_page_1 não pode entrar aqui
                                    if i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual and intersects_json(bboxes_celulas_todas[i-1], bboxes_textos_todos[j][0]) == False:
                                        print('entrou')
                                        word_to_add = bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        col_idx+=1

                                    # a coluna anterior verdadeira interseta-se o OCR atual e o OCR atual interseta-se com a coluna atual
                                    cabec_anterior_verdadeiro_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i-1], bboxes_textos_todos[j][0])
                                    print(bboxes_textos_todos[j][1])
                                    if cabec_anterior_verdadeiro_interseta_com_ocr_atual==True and i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual:
                                        word_to_add = bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx-1)] = word_to_add
                                        col_idx+=1

                                    # para os casos em que vem a seguir de uma coisa com inicio de OCR junto (ver se faz sentido restringir assim ou não)
                                    if i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual and bbox_seguinte_intersetase_com_cabecalho_seguinte == False:
                                        col_idx += 1
                                        word_to_add = palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        ocupado = True
                                        col_idx+=1

                                    # quando o conteudo de mais do que uma célula do cabeçalho está na primeira caixa de OCR
                                    if i == 0 and cabec_anterior_interseta_com_ocr_atual and cabec_seguinte_interseta_com_ocr_atual and cabec_seguinte_interseta_com_ocr_atual:
                                        print('entrou')
                                        celulas_textos[(0, col_idx)] = palavra
                                        col_idx+=1
                                        if palavra == palavras[-1]:
                                            col_idx-=1


                                    # quando a palavra se interseta com a coluna anterior preciso de garantir que a prox bbox n fica vazia !!!!!!!!!!!!!!!!!!!!!!!
                                    # interfersa_2_page_2 não entra nesta condição
                                    # se a proxima bbox de texto não se interseta com a bbox de célula seguinte
                                    elif i < len(bboxes_celulas_todas)-2 and j < len(bboxes_textos_todos)-2 and not intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]) and intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and ocupado == False:
                                        celulas_textos[(0, col_idx+1)] = palavra
                                        ocr_partido = True

                                    # se a caixa de OCR anterior se conectar com a célula em que queremos adicionar o texto
                                    else:
                                        if levou_espaco_e_juntou_com_a_coluna_anterior:
                                            col_idx -=1
                                        if iterador == 0 and levou_espaco_e_juntou_com_a_coluna_anterior == False:
                                            col_idx -= 1
                                        if (0, col_idx) in celulas_textos and ocupado == False and len(celulas_textos):
                                            col_idx += 1
                                            celulas_textos[(0, col_idx)] = palavra
                                            ocr_partido = True
                                            levou_espaco_e_juntou_com_a_coluna_anterior = False
                                        if len(celulas_textos) > count_headers:
                                            celulas_textos[(0, col_idx-1)] = celulas_textos[(0, col_idx-1)]+' '+palavra
                                    # quando se interseta com a coluna anterior
                                    if intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0:
                                        col_idx += 1
                                    palavras_adicionadas.add(palavra)
                                    iterador +=1

                        else: # só tem uma interseção
                            key = (0, id_celula_linha_coluna[i][1])
                            if key not in celulas_textos:
                                celulas_textos[key] = bboxes_textos_todos[j][1]
                                levou_espaco_e_juntou_com_a_coluna_anterior = False
                                coluna_atual = i
                            else:
                                celulas_textos[key] += ' ' + bboxes_textos_todos[j][1]
                                levou_espaco_e_juntou_com_a_coluna_anterior = True
                                coluna_atual = i

            else:  # Outras células
                # criar a lista de bounding boxes de ocr, sem texto
                bboxes_ocr_sem_texto = []
                for j in range(len(bboxes_textos_todos)):
                    bboxes_ocr_sem_texto.append(bboxes_textos_todos[j][0])
                print(bboxes_ocr_sem_texto)

                for j in range(len(bboxes_textos_todos)):
                    if isinstance(bboxes_textos_todos[j][0], (list, tuple)) and intersects_content(bboxes_celulas_todas[i], bboxes_textos_todos[j][0]):
                        key = (id_celula_linha_coluna[i][0], id_celula_linha_coluna[i][1])
                        # se o ocr se intersetar com mais do que uma célula
                        if contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[0] > 1:
                            # extrai-se o texto presente nessa caixa de texto
                            string = bboxes_textos_todos[j][1]
                            print(string)
                            if string == string_anterior:
                                continue
                            string_anterior = string
                            if string == '12/015924 SU REFERENCIA':
                                print('entrou')

                            print(i+contar_intersecoes(bboxes_textos_todos[j][0], bboxes_celulas_todas)-1)
                            celula_a_direita = bboxes_celulas_todas[i+contar_intersecoes(bboxes_textos_todos[j][0], bboxes_celulas_todas)-1]
                            proxima_ocr = []
                            if j < len(bboxes_textos_todos)-1:
                                proxima_ocr.append(bboxes_textos_todos[j+1][0])
                                print(type(proxima_ocr))
                                # verificar se a ultima celula de intersecao do ocr não se interseta com o ocr seguinte
                                if contar_intersecoes(celula_a_direita, proxima_ocr) == 0:
                                    palavras_linhas = dividir_string_por_proporcoes(string, contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[1])

                                # verificar se a ultima celula de intersecao com o ocr se interseta com um proximo ocr
                                elif contar_intersecoes(celula_a_direita, proxima_ocr) > 0:
                                    print(bboxes_textos_todos[j][0])
                                    print(remover_ultima_somar_penultima(contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[1]))
                                    resultado = contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)
                                    proporcoes = remover_ultima_somar_penultima(resultado[1])
                                    palavras_linhas = dividir_string_por_proporcoes(string, proporcoes)
                            col_idx = id_celula_linha_coluna[i][1]
                            for palavra_linha in palavras_linhas:
                                print(palavra_linha)
                                key = (id_celula_linha_coluna[i][0], col_idx)
                                if key not in celulas_textos:
                                    celulas_textos[key] = palavra_linha

                                    col_idx +=1
                                elif key in celulas_textos:
                                    print(type(celulas_textos[key]))
                                    celulas_textos[key] += ' ' + palavra_linha

                        else:
                            if key not in celulas_textos:
                                celulas_textos[key] = bboxes_textos_todos[j][1]
                            else:
                                celulas_textos[key] += ' ' + bboxes_textos_todos[j][1]

        # Ajustar textos duplicados antes de processar
        celulas_textos_ordenado = {k: celulas_textos[k] for k in sorted(celulas_textos.keys(), key=lambda x: (x[1], x[0]))}

        for key, text in celulas_textos_ordenado.items():
            row, col = key
            if row < len(y_values) - 1 and col < len(x_values) - 1:
                i = row * (len(x_values) - 1) + col
                bbox = bboxes_celulas_todas[i]
                dados_formatados_para_1_fornecedor.append([text, row, col, bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1], [bbox[0], bbox[1]], [bbox[2], bbox[3]]])

        json_data = converter_para_json(dados_formatados_para_1_fornecedor, 'tabela_resultante')
        create_excel_from_json(json_data, 'tabela_resultante')

def somar_canto_sup_esq_da_previsao(resized_lista_coords_x_y, pred):
    # Extrair coordenadas X e Y da lista
    coords_x = resized_lista_coords_x_y[0]
    coords_y = resized_lista_coords_x_y[1]

    # Extrair os valores de pred
    valor_x = pred[0][0]
    valor_y = pred[0][1]

    # Adicionar os valores de pred às coordenadas de X e de Y
    novas_coords_x = [x + valor_x for x in coords_x]
    novas_coords_y = [y + valor_y for y in coords_y]

    # Retornar a lista ajustada
    return [novas_coords_x, novas_coords_y]

def transformar_para_resultados_ocr(only_inside_ocr_bboxes):
    # Criar uma nova lista para guardar os resultados transformados
    ocr_results = []

    # Itera sobre cada dicionário na lista original
    for item in only_inside_ocr_bboxes:
        # Extrair o boundingBox e o text do dicionário
        bbox = item['boundingBox']
        text = item['text']
        # Adicionar uma lista [bbox, text] na lista de resultados
        ocr_results.append([bbox, text])

    # Retornar a nova lista transformada
    return ocr_results

if __name__== "__main__":

    inicio = time.time()
    caminho_imagem = 'data_input/todas_as_imagens_de_fornecedores_orientacao_correta/12012023090841_page_34.jpg'
    # Abrir e redimensionar a imagem para 2400, porque é recomendado pela Microsoft para bons resultados do OCR
    image_2400 = load_and_resize_image(caminho_imagem, 2400)[0]

    # Converter a imagem redimensionada para bytes
    image_2400_bytes = pil_image_to_bytes(image_2400)

    # Aplicar o Azure OCR
    text_result, angle = azure_api.get_ocr_azure(image_2400_bytes) # Resultados OCR, ângulo de rotação

    azure_api.annotate_image(image_2400, text_result, 'output_data/12012023090841_page_34.jpg')

    # Converter o formato da imagem para PIL e Rodar com o angulo obtido com o OCR
    imagem_rodada = rotate_image(image_2400, angle)[0]
    ## anotar a imagem com as bounding boxes das deteções de OCR originais em toda a imagem
    annotate_image_with_inclined_bounding_boxes(imagem_rodada, rotate_bounding_boxes(image_2400_bytes, text_result, -angle))
    print(rotate_bounding_boxes(image_2400_bytes, text_result, -angle))

    # Redimensionar a imagem para 640 por 640 para ser sujeita a previsão pela rede neuronal
    imagem = ImageOps.pad(imagem_rodada, (640, 640), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')
    # guardar a imagem para efeitos de verificação
    imagem.save( 'output_data/tableDetection/imagem_640.jpeg', format='JPEG')

    # Obter a imagem já redimensionada para 2400 por 2400 para aplicar OCR no final
    imagem_2400 = ImageOps.pad(imagem_rodada, (2400, 2400), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')
    imagem_2400.save('output_data/tableDetection/imagem_2400.jpeg', format='JPEG')

    # modelo YOLOv8 treinado com 250 épocas para a deteção de tabelas
    model = YOLO('runs_small_dataset_2/weights/best.pt')

    # Previsão da localização da tabela na imagem, com o modelo treinado
    pred = predict_bboxes(imagem, model)

    if not pred:
        # Se não tiver previsão, a metodologia é interrompida.
        print('Não foi possível obter previsões.')

    else:
        # recortar a imagem, eliminar a parte da imagem que se encontra fora da caixa delimitadora da tabela prevista
        imagem_recortada = crop(imagem_2400, pred[0])
        # Guardar a imagem recortada tendo em conta a previsão, para efeitos de verificação
        imagem_recortada.save('output_data/tableDetection/resultado_imagem_recortada_codigo_completo.jpeg')

        # Resultados do OCR dentro da bounding box prevista
        only_inside_ocr_bboxes = filter_bboxes_inside_predicted_region(rotate_bounding_boxes(image_2400_bytes, text_result, -angle), pred[0])
        print(only_inside_ocr_bboxes)
        print(len(text_result))
        print(len(only_inside_ocr_bboxes))
        print('sem bounding boxes de texto detetado pelo OCR fora da região prevista', only_inside_ocr_bboxes)

        annotate_image_with_inclined_bounding_boxes(imagem_2400, only_inside_ocr_bboxes)

        adjusted_bboxes = adjust_bboxes_to_pred(only_inside_ocr_bboxes, pred[0])

        converted_adjusted_bboxes = convert_to_rectangular_format(adjusted_bboxes)
        print('coordenadas convertidas', converted_adjusted_bboxes)

        # Conversão da imagem recortada para dimensões 640 por 640 para a próxima etapa de divisão da estrutura da tabela
        # podemos fazer a conversão assim porque a dimensão maior da imagem recortada é sempre superior a 640.
        # converter a imagem recortada para um formato 640 por 640 (resultados melhores na rede neuronal que segue)
        imagem_recortada_640 = redimensionar_imagem(imagem_recortada)

        # Carregar o modelo treinado para detetar os cabeçalhos das tabelas
        model_c = YOLO('runs_interest_1000_cabecalho_200_epochs/weights/best.pt')

        # Previsão das bounding boxes de cabeçalhos nas imagens 640
        bboxes_scores = predict_bboxes_c_l(imagem_recortada_640, model_c)

        save_annotated_image(imagem_recortada_640, bboxes_scores, 'previsao_cabecalhos_.jpg')

        # Guardar os Resultados da Previsão do modelo treinado para os cabeçalhos
        results = []
        for bbox, score in bboxes_scores:
            x1, y1, x2, y2 = bbox
            # Alteração para o formato que facilita o desenho da linha vertical de uma ponta à outra da imagem, tendo em conta as coordenadas de bboxes previstas
            transformed_bbox = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            results.append((transformed_bbox, score))
        print(results)

        # Lista com coordenadas de y do canto inferior esquerdo de cada bounding box prevista para cabeçalho
        final_results = []

        # Itera sobre os resultados para extrair as coordenadas
        for bbox, _ in results:
            # bbox tem a forma [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            # y do canto inferior esquerdo é o segundo elemento de bbox[3], que é [x1, y2]
            final_results.append(bbox[3][1])
        print(final_results)

        ########### Coordenadas do canto superior esquerdo para ver qual é a mínima e ser o topo da linha dos cabeçalhos
        # Lista para armazenar os resultados finais com o nome do arquivo e as coordenadas de y do canto superior esquerdo de cada bounding box
        final_results_upper_left = []

        for bbox, _ in results:
            # bbox tem a forma [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            # y do canto superior esquerdo é o segundo elemento de bbox[0], que é [x1, y1]
            print(bbox[0][1])
            final_results_upper_left.append(bbox[0][1])

        print(final_results_upper_left)

        ####### Obter o valor de y mínimo para o cabeçalho
        # Itera sobre os resultados finais para calcular a média das coordenadas de y
        if final_results_upper_left:  # Verifica se a lista não está vazia
            # Calcula a média das coordenadas de y
            y_min = min(final_results_upper_left)
            final_results_minimo_cabecalho = int(y_min)  # Converte a média para inteiro para uso na função de desenho
        else:
            # Se não houver coordenadas, podemos definir um valor padrão ou simplesmente omitir
            final_results_minimo_cabecalho = None

        print(final_results_minimo_cabecalho)

        # Lista para armazenar os resultados com a média das coordenadas de y para cada imagem
        if final_results:  # Verifica se a lista não está vazia
            # Calcula a média das coordenadas de y inferior, para definir a parte de baixo do cabeçalho
            final_results_medias = int(np.mean(final_results))
        else:
            # Se não houver coordenadas, definir um valor padrão none
            final_results_medias= None

        print(final_results_medias)

        output_directory_nms = 'output_data/tableStructure'

        vertical_apos_nms = process_images_with_nms_and_vertical_lines(results, imagem_recortada_640, output_directory_nms)
        print('vanms',vertical_apos_nms)

        coordenadas_x = []

        for bbox in vertical_apos_nms:
            coordenadas_x.extend([bbox[0], bbox[2]])  # Adiciona o y_min e o y_max de cada bounding box

        # Imprimir a lista coordenadas_x
        print('coordenadas_x',coordenadas_x)

        x_values = sorted(coordenadas_x)  # Primeiro ordenamos os valores para garantir comparações corretas
        coords_x = []
        i = 0
        # se a distancia entre dois valores ordenados de x for inferior a 20, remove-se o valor menor de x, entre os dois
        while i < len(x_values):
            # distancia até 20 pixels para considerar a linha vertical que se segue na divisão das colunas
            if i + 1 < len(x_values) and (x_values[i + 1] - x_values[i] < 20):
                # Se a diferença entre valores consecutivos é menor que 5, remove da lista o menor valor (ou seja, i, já que estão ordenados)
                coords_x.append(x_values[i + 1])  # Adiciona o maior dos dois valores de x
                i += 2  # Avança dois passos para evitar a dupla contagem
            else:
                coords_x.append(x_values[i])
                i += 1

        # Certifica-se de que o último valor seja considerado caso não fosse parte de um par comparado
        if len(x_values) > 1 and i == len(x_values) - 1 and abs(x_values[-1] - coords_x[-1]) >= 20:
            coords_x.append(x_values[-1])

        # Imprime a lista coords_y para ver - Estas são as coordenadas para juntar
        print('coordenadas de x', coords_x)

        # Adiciona o valor 0 no início se o primeiro valor de x for maior que 20
        if coords_x[0] > 20:
            coords_x.insert(0, 0)

        # Adiciona 640 no final se o último valor de x for menor que 620
        if coords_x[-1] < 620:
            coords_x.append(640)

        adjusted_coords_x = coords_x

        # Imprime a lista de coordenadas ajustadas para visualização
        print('Coordenadas ajustadas de x:', adjusted_coords_x)

        # Utiliza 'adjusted_coords_x' para criar os arrays finais e processar as imagens
        # array_x_lista apenas serve para representar as delimitações obtidas no final deste processo
        array_x_lista = []

        for j in range(len(adjusted_coords_x)-1):
            array_x_lista.append(np.array([adjusted_coords_x[j], 0, adjusted_coords_x[j+1], 640]))

        # Impressão final dos resultados para confirmar a estrutura correta dos dados
        print('Lista final de arrays x:', array_x_lista)

        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)
        for box in array_x_lista:
            print(box)
            # Desenha cada bounding box
            start_point = (box[0], box[1])  # Coordenadas do canto superior esquerdo
            end_point = (box[2], box[3])  # Coordenadas do canto inferior direito
            color = (255, 0, 0)  # Cor da bounding box em BGR (azul)
            thickness = 1  # Espessura da linha da bounding box
            cv2.rectangle(image, start_point, end_point, color, thickness)

        # Guarda a imagem anotada na pasta de output
        save_path = os.path.join('output_data/tableStructure/imagem_recortada_640_linhas_verticais.jpg')
        cv2.imwrite(save_path, image)

        # Carregar o modelo treinado para a deteção da altura das linhas
        model_l = YOLO('runs_interest_1000_altura_linhas_249_epochs/weights/best.pt')

        results_h = []

        bboxes_scores = predict_bboxes_c_l(imagem_recortada_640, model_l)

        save_annotated_image(imagem_recortada_640, bboxes_scores, 'previsao_altura_linhas')

        for bbox, score in bboxes_scores:
            x1, y1, x2, y2 = bbox
            # Alteração para o formato que permite a representação da linha vertical de uma ponta à outra da imagem, tendo em conta as coordenadas de bboxes previstas
            transformed_bbox = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            results_h.append((transformed_bbox, score))
        print(results_h)

        horizontal_apos_nms = process_images_with_nms_and_horizontal_lines(results_h, imagem_recortada_640)
        print(horizontal_apos_nms)

        coordenadas_y = []

        for bbox in horizontal_apos_nms:
            coordenadas_y.extend([bbox[1], bbox[3]])  # Adiciona y_min e y_max de cada bounding box

        print('coordenadas_y', coordenadas_y)

        '''Parte mais difícil'''

        ####### Experimentar com os limites inferior e superior do cabeçalho ######

        # Tenta abrir a imagem para obter a sua altura
        image_height = imagem_recortada_640.height

        # Encontra a média de y para o cabeçalho desta imagem em final_results_medias

        header_y = final_results_medias

        # Se header_y não foi encontrado ou a lista de y_values está vazia, utiliza um valor padrão de 0
        if header_y is None:
            print(f"Aviso: Coordenada de divisão de cabeçalho não encontrada. Utiliza-se o valor padrão.")
            header_y = 0  # ou continue para omitir o arquivo

        header_begin = final_results_minimo_cabecalho


        # Se header_y não foi encontrado ou a lista de y_values está vazia, utiliza um valor padrão igual a 0
        if header_begin is None:
            print(f"Aviso: Coordenada de divisão de cabeçalho não encontrada. Utiliza-se o valor padrão.")
            header_begin = 0

        # Adiciona a média de y para o final vertical do cabeçalho e o minimo do inicio vertical do cabeçalho à lista de y_values e ordena
        y_values = sorted(coordenadas_y + [header_y]+[header_begin])

        new_y_values = []
        i = 0
        while i < len(y_values):
            if i + 1 < len(y_values) and (y_values[i + 1] - y_values[i] < 6):
                new_y_values.append(y_values[i + 1])  # Adiciona o maior dos dois valores para a coordenada de y
                i += 2  # Avança dois passos para evitar dupla contagem
            else:
                new_y_values.append(y_values[i])
                i += 1

        # Certifica-se de que o último valor seja considerado se não for parte de um par comparado
        if len(y_values) > 1 and i == len(y_values) - 1 and abs(y_values[-1] - new_y_values[-1]) >= 6:
            new_y_values.append(y_values[-1])

        # Se o último valor de y é menor que a altura da imagem em menos de 40 pixels, adiciona a altura da imagem
        if new_y_values[-1] < (image_height - 40):
            new_y_values.append(image_height)

        coords_y_div_cabecalho_inteiro = new_y_values

        print('coords_y_div_cabecalho_inteiro',coords_y_div_cabecalho_inteiro)

        '''Fim da parte mais difícil'''

        array_y_lista_div_cabecalho_inteiro = []
        for j in range(len(coords_y_div_cabecalho_inteiro)-1):
            array_y_lista_div_cabecalho_inteiro.append(np.array([0, coords_y_div_cabecalho_inteiro[j], 640, coords_y_div_cabecalho_inteiro[j+1]]))
        print('array_y_lista',array_y_lista_div_cabecalho_inteiro)


        # Processamento de cada imagem e bounding box
        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)
        if image is not None:
            for box in array_y_lista_div_cabecalho_inteiro:
                # Desenho de cada bounding box
                start_point = (box[0], box[1])  # Coordenadas do canto superior esquerdo
                end_point = (box[2], box[3])  # Coordenadas do canto inferior direito
                color = (255, 0, 0)  # Cor da bounding box em BGR (azul)
                thickness = 1  # Espessura da linha da bounding box
                cv2.rectangle(image, start_point, end_point, color, thickness)

            # Guardar a imagem anotada na pasta de saída relativa ao reconhecimento da estrutura tabela
            cv2.imwrite('output_data/tableStructure/recortada_anotada_com_linhas.jpg', image)
        else:
            print(f'Imagem não encontrada.')

        # Criar uma nova lista que combina as coords_x e coords_y
        lista_coords_x_y = [coords_x, coords_y_div_cabecalho_inteiro]
        print(len(coords_y_div_cabecalho_inteiro))
        print('lista_coords_x_y', lista_coords_x_y)

        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)

        if image is None:
            print(f"Erro ao carregar a imagem.")

        # Desenhar linhas verticais
        for x in lista_coords_x_y[0]:
            cv2.line(image, (x, 0), (x, image.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in lista_coords_x_y[1]:
            cv2.line(image, (0, y), (image.shape[1], y), (0, 255, 0), 2)  # Linhas verdes

        # Guardar a imagem anotada
        cv2.imwrite('output_data/tableStructure/anotada_linhas_verticais_horizontais_640.jpg', image)
        print(f"Imagem anotada guardada.")

        # Criar a lista com as coordenadas redimensionadas
        resized_lista_coords_x_y = []
        width_640, height_640 = get_image_dimensions(imagem_recortada_640)
        width_grandes, height_grandes = get_image_dimensions(imagem_recortada)
        resized_x_values = [x * (width_grandes/width_640) for x in lista_coords_x_y[0]]
        resized_y_values = [y * (height_grandes/height_640) for y in lista_coords_x_y[1]]
        print(len(resized_y_values))
        resized_lista_coords_x_y = [resized_x_values, resized_y_values]
        print(resized_lista_coords_x_y)

        if isinstance(imagem_recortada, Image.Image):
            image = pil_to_cv2(imagem_recortada)

        if image is None:
            print(f"Erro ao carregar a imagem.")
        # Desenhar as linhas verticais
        for x in resized_lista_coords_x_y[0]:
            cv2.line(image, (int(x), 0), (int(x), image.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar as linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(image, (0, int(y)), (image.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes

        print('pred', pred)
        # Guardar a imagem anotada
        cv2.imwrite("output_data/tableStructure/linhas_horizontais_verticais_redimensionadas.jpg", image)
        print(f"Imagem anotada guardada")

        # ocr_results para 1 imagem
        # somar a esta lista, nas coordenadas de x o valor de x do canto superior esquerdo da previsão
        # somar a esta lista, nas coordenadas de y o valor de y do canto superior esquerdo da previsão
        resized_lista_coords_x_y = somar_canto_sup_esq_da_previsao(resized_lista_coords_x_y, pred)
        print('resized_lista_x_y', resized_lista_coords_x_y)

        if isinstance(imagem_rodada, Image.Image):
            image_ = pil_to_cv2(imagem_rodada)

        if image_ is None:
            print(f"Erro ao carregar a imagem.")
        # Desenhar as linhas verticais
        for x in resized_lista_coords_x_y[0]:
            cv2.line(image_, (int(x), 0), (int(x), image_.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar as linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(image_, (0, int(y)), (image_.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes

        print('pred', pred)
        # Guardar a imagem anotada
        cv2.imwrite("output_data/tableExtraction/translacao_coordenadas_celulas.jpg", image_)
        print(f"Imagem anotada guardada")

        # falta determinar o ocr_results para 1 imagem
        ocr_results = transformar_para_resultados_ocr(only_inside_ocr_bboxes)

        # Exibir os resultados
        print(ocr_results)

        image_path = 'output_data_ocr_twice/tableDetection/imagem_2400.jpeg'
        output_path = 'imagem_anotada_ocr_rodado.jpg'
        annotate_image(image_path, ocr_results, output_path)

        resized_lista_coords_x_y = sorted(resized_lista_coords_x_y, key=lambda x: x[0])
        print(len(resized_lista_coords_x_y))
        print(len(resized_lista_coords_x_y[1]))
        ocr_results = sorted(ocr_results, key=lambda x: x[0])

        new_resized_list_1 = remove_bboxes_without_ocr(ocr_results, resized_lista_coords_x_y, caminho_imagem)
        print(len(new_resized_list_1[1]))
        lista_sem_linha_final_amais_1= remove_unneeded_y_coords(ocr_results, new_resized_list_1)
        print(len(lista_sem_linha_final_amais_1))
        print(len(lista_sem_linha_final_amais_1[1]))
        create_excel_and_json(lista_sem_linha_final_amais_1, ocr_results)

        fim = time.time()
        tempo_execucao=fim-inicio
        # determinação do tempo de execução
        print(f"Tempo de execução: {tempo_execucao:.6f} segundos")
