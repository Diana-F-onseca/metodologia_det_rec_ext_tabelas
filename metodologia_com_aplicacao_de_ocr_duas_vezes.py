# -*- coding: utf-8 -*-
"""metodologia_com_aplicacao_de_ocr_duas_vezes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oapDVZWiDG9EmB4LW_YAbmf3wktJUtYY
"""

# Importação das bibliotecas
import cv2
from PIL import Image, ImageOps, ImageDraw, ImageFont
import numpy as np
from ultralytics import YOLO
from pathlib import Path
import io
import requests
import time
import numpy as np
import tensorflow as tf
import os
import math
from statistics import mode, StatisticsError


import json

from modules_1_image.divide_info_1 import *
from modules_1_image.ocr_manipulation import *
from modules_1_image.text_list_manipulation_1 import *
from modules_1_image.coor_manipulation_1 import *
from modules_1_image.excel_json_manipulation_1 import *


# Definição da classe AzureOCRApi (fornecida)

class AzureOCRApi():
    def __init__(self, endpoint, subscription_key):
        self.endpoint = endpoint
        self.subscription_key = subscription_key

    def _post_analyze(self, image_data):
        ocr_url = f"{self.endpoint}/read/analyze"
        headers = {'Ocp-Apim-Subscription-Key': self.subscription_key,
                   'Content-Type': 'application/octet-stream'}
        params = {'detectOrientation': 'true'}
        response = requests.post(ocr_url, headers=headers, params=params, data=image_data)
        response.raise_for_status()
        return response

    def _read_analyze(self, response):
        read_operation_location = response.headers["Operation-Location"]
        operation_id = read_operation_location.split("/")[-1]
        ocr_url = f"{self.endpoint}/read/analyzeResults/{operation_id}"
        headers = {'Ocp-Apim-Subscription-Key': self.subscription_key, 'Accept': 'application/json'}
        response = requests.get(ocr_url, headers=headers)
        response.raise_for_status()
        return response.json()

    def _read_results(self, results: dict):
        text_total = []
        angle = results['analyzeResult']['readResults'][0]['angle']

        # Iterar por cada resultado de leitura
        for text_result in results['analyzeResult']['readResults']:
            for line in text_result['lines']:
                # A bounding box vem em formato de lista [x1, y1, x2, y2, x3, y3, x4, y4]
                boundingBox = line['boundingBox']
                text_total.append({
                    'text': line['text'],
                    'boundingBox': boundingBox,  # Adiciona a bounding box ao resultado
                    'conf': line.get('appearance', {}).get('style', {}).get('confidence', 1)
                })
        return text_total, angle


    def get_ocr_azure(self, file):
        response = self._post_analyze(image_data=file)
        while True:
            results = self._read_analyze(response=response)
            if results['status'] not in ['notStarted', 'running']:
                break
            time.sleep(0.5)
        text_result, angle = self._read_results(results)
        return text_result, angle


def annotate_image( image, text_result, output_path, color):
    draw = ImageDraw.Draw(image)
    font = ImageFont.load_default()

    for item in text_result:
        text = item['text']
        bbox = item['boundingBox']

        # Desenhar a caixa delimitadora
        draw.line([(bbox[i], bbox[i+1]) for i in range(0, len(bbox), 2)] + [(bbox[0], bbox[1])],
                    fill=color, width=2)

        # Calcular a posição para escrever o texto
        text_position = (bbox[0], bbox[1] - 10)  # Texto ligeiramente acima da caixa

        # Escrever o texto
        draw.text(text_position, text, fill=color, font=font)

    # Guardar a imagem anotada
    image.save(output_path)
    print(f"Imagem anotada guardada em: {output_path}")

# Método que converte a imagem em bytes
def image_to_bytes(image_cv):
    # Usar o imencode para converter a imagem OpenCV num formato de memória (array de bytes)
    _, buffer = cv2.imencode('.jpg', image_cv)  # Codifica a imagem como JPEG
    io_buf = io.BytesIO(buffer)  # Cria um buffer de bytes
    return io_buf.getvalue()  # Retorna os bytes da imagem


# Redimensiona a imagem, com respeito às suas proporções originais, para as dimensões pretendidas
# O método chama e redimensiona a imagem para 2400x2400, com o PIL.
def load_and_resize_image(image_path, size):
    image = Image.open(image_path)
    image_resized = ImageOps.pad(image, (size, size), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')
    return image_resized, image_resized.width, image_resized.height

# Rotação da imagem, com o valor de ângulo retornado pelo OCR Azure
# A imagem colocada como parâmetro neste método tem de estar em formato PIL.
# O parâmetro angle corresponde ao valor de ângulo com o qual as imagens vão ser rodadas
# Retorna a imagem rodada como um array numpy.
def rotate_image(image_file, angle):
    rotated_image = image_file.rotate(angle, expand=False)

    return rotated_image, rotated_image.width, rotated_image.height


# Previsão da bounding box de maior score na imagem, que retorna a bounding box redimensionada para uma imagem equivalente de dimensões 2400 por 2400 prevista e o valor de Score da previsão
def predict_bboxes(image, model):
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    predictions = model(image_cv, imgsz=640)
    bboxes = []
    if hasattr(predictions[0].boxes, 'xyxy') and hasattr(predictions[0].boxes, 'conf'):
        for i, box in enumerate(predictions[0].boxes.xyxy):
            bboxes.append((box.tolist(), predictions[0].boxes.conf[i].item()))

    if not bboxes:
        print('Sem Previsão de Tabela')
        return
    else:
        sorted_pred = sorted(bboxes, key=lambda x: x[1])
        # box_pred corresponde à bounding box com maior valor de score na previsão
        box_pred = list(sorted_pred[0][0])
        box_pred_2400 = [box_pred[0]*(2400/640),box_pred[1]*(2400/640),box_pred[2]*(2400/640),box_pred[3]*(2400/640)]
        # print('Coordenadas da bounding box prevista', box_pred_2400)
        score_pred = np.round(float(sorted_pred[0][1]),3)
        # print('Score da bounding box prevista:', score_pred)
        return box_pred_2400, score_pred # pred[0][0] x_cse e pred[0][1] y_cse

# desenha as caixas delimitadoras e imprime os valores de scores numa imagem no formato PIL e guarda a imagem anotada
# recebe uma imagem no formato PIL, a bboxes_scores, que é uma lista de tuplos com as caixas delimitadoras e o valor de confiança a elas associado
# rece ainda o nome do ficheiro, que corresponde ao caminho original da imagem, para extrair o nome do ficheiro
def save_annotated_image(image, bboxes_scores, nome_ficheiro):
    # Converte a imagem PIL para o formato OpenCV
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    for bbox, score in bboxes_scores:
        x1, y1, x2, y2 = bbox
        cv2.rectangle(image_cv, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Borda vermelha
        cv2.putText(image_cv, f'Score: {score:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    string = nome_ficheiro + '.jpg'
    # Guarda a imagem anotada
    cv2.imwrite(os.path.join(string), image_cv)

# Método que guarda (e mostra) a imagem recortada
def crop(image, bbox):

    if not bbox:
        print("Não foi encontrada nenhuma caixa delimitadora.")
        return None, "No bounding boxes found", None

    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    x1, y1, x2, y2 = map(int, bbox)
    cropped_image_cv = image_cv[y1:y2, x1:x2]

    # Converte a imagem para um objeto PIL, para que possa ser guardada
    cropped_image_pil = Image.fromarray(cv2.cvtColor(cropped_image_cv, cv2.COLOR_BGR2RGB))

    return cropped_image_pil


# criar uma extensão da classe com a chave e url fornecidos para possibilitar a utilização do OCR do Azure
azure_api = AzureOCRApi('https://westeurope.api.cognitive.microsoft.com/vision/v3.2', 'b700dee2252d45e589852898cc6b56a0')

# 2400x2400
# OCR - angulo e texto
# Rodar e caso ter de rodar a imagem, ajustar as coordenadas do OCR
# 640x640
# chamar previsao das tabelas
# região de interesse - coordenadas
# converter para 2400 por 2400 (coordenadas)
# Filtro do OCR para as coordenadas da tabela
# Rotação das coordenadas do OCR


# método que realiza a rotação de um ponto com o valor do ângulo passado como parâmetro
def rotate_point(cx, cy, angle, px, py):
    """Roda um ponto em torno de outro ponto (cx, cy) por um ângulo cujo valor é passado como parâmetro."""
    # conversão dos ângulos em radianos, porque as funções trigonométricas recebem radianos
    seno = math.sin(math.radians(angle))
    cosseno = math.cos(math.radians(angle))

    # Translada o ponto de volta à origem:
    # Subtrai as coordenadas do ponto central (cx, cy) das coordenadas do ponto (px, py). Faz com que o centro (cx, cy) corresponda à origem (0,0).
    px -= cx
    py -= cy

    # Roda o ponto
    # Calcula as novas coordenadas do ponto rodado, com as fórmulas de rotação:
    xrot = px * cosseno - py * seno
    yrot = px * seno + py * cosseno

    # Translada o ponto tendo em conta as coordenadas de centro originais:
    px = xrot + cx
    py = yrot + cy
    return int(px), int(py)

# converter as imagens de bytes para objeto do tipo PIL
def bytes_to_pil(image_bytes):
    """Converte bytes de imagem para um objeto PIL.Image."""
    image_stream = io.BytesIO(image_bytes)
    image = Image.open(image_stream)
    return image

# Rodar as caixas delimitadoras
def rotate_bounding_boxes(image, annotations, angle):
    # o ângulo chamado é o simétrico do angulo que se vai rodar
    if isinstance(image, bytes):
       image = bytes_to_pil(image)
    # Centro da imagem para rotação
    height, width = image.height, image.width

    cx, cy = width // 2, height // 2

    # Novas anotações após a rotação das caixas
    new_annotations = []

    for annotation in annotations:
        # Obter os pontos da bounding box atual
        points = annotation['boundingBox']
        rotated_points = []

        # pontos no formato [x1, y1, x2, y2, x3, y3, x4, y4]
        # o ciclo é percorrido de 2 em 2
        for i in range(0, len(points), 2):
            px, py = points[i], points[i+1]
            rotated_px, rotated_py = rotate_point(cx, cy, angle, px, py)
            rotated_points.extend([rotated_px, rotated_py])

        # Guardar a nova bounding box rotacionada
        new_annotations.append({
            'text': annotation['text'],
            'boundingBox': rotated_points,
            'conf': annotation['conf']
        })

    return new_annotations

# converte o formato de 4 cantos para o canto superior esquerdo e o canto inferior direito
def convert_bounding_boxes(data):
    text_ocr_to_images = []
    for item in data:
        text = item['text']
        conf = item['conf']
        # Extraído de cada bloco: [x1, y1, x2, y2, x3, y3, x4, y4]
        bounding_box = item['boundingBox']
        # Cálculo dos valores mínimos e máximos de x e y
        x_min = min(bounding_box[0], bounding_box[2], bounding_box[4], bounding_box[6])
        y_min = min(bounding_box[1], bounding_box[3], bounding_box[5], bounding_box[7])
        x_max = max(bounding_box[0], bounding_box[2], bounding_box[4], bounding_box[6])
        y_max = max(bounding_box[1], bounding_box[3], bounding_box[5], bounding_box[7])
        # Criação de uma nova caixa delimitadora
        new_bounding_box = [x_min, y_min, x_max, y_max]
        # Adiciona o novo item do dicionário
        text_ocr_to_images.append({
            'text': text,
            'boundingBox': new_bounding_box,
            'conf': conf
        })
    return text_ocr_to_images

def bytes_to_cv2(image_bytes):
    # Converte os bytes da imagem para um array numpy
    numpy_array = np.frombuffer(image_bytes, np.uint8)
    # Ler a imagem como BGR (padrão do OpenCV)
    cv2_image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
    return cv2_image

def pil_to_cv2(pil_image):
    # Converter PIL Image para um array NumPy
    numpy_image = np.array(pil_image)
    # Converter de RGB para BGR
    cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)
    return cv2_image

def annotate_image_with_inclined_bounding_boxes(image, annotations):
    if isinstance(image, bytes):
        # Converter de bytes para OpenCV
        image = bytes_to_cv2(image)
        print("Imagem processada como bytes.")
    elif isinstance(image, Image.Image):
        # Converter de PIL para OpenCV
        image = pil_to_cv2(image)
        print("Imagem processada como PIL Image.")
    if image is None:
        print("Erro ao carregar a imagem.")
        return

    # Configurações para o texto
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    font_color = (255, 255, 255)  # Branco
    font_thickness = 1
    box_color = (0, 255, 0)  # Verde
    box_thickness = 2

    for annotation in annotations:
        # Formato de caixa delimitadora com oito valores x, y para quatro pontos
        pts = np.array([
            annotation['boundingBox'][0:2],
            annotation['boundingBox'][2:4],
            annotation['boundingBox'][4:6],
            annotation['boundingBox'][6:8]
        ], np.int32)
        pts = pts.reshape((-1, 1, 2))

        # Representar a bounding box inclinada obtida, na imagem
        cv2.polylines(image, [pts], isClosed=True, color=box_color, thickness=box_thickness)

        # Calcular a posição do texto (na média das coordenadas x e y dos pontos)
        text_x = int(np.mean(pts[:, 0, 0]))
        text_y = int(np.mean(pts[:, 0, 1]))

        # Desenhar o texto
        cv2.putText(image, annotation['text'], (text_x, text_y), font, font_scale, font_color, font_thickness)

    # Guardar a imagem anotada
    cv2.imwrite('output_data_ocr_twice/tableStructure/tabela_anotada_com_as_bboxes_rodadas.jpg', image)
    print("Imagem anotada guardada na diretoria 'annotated_rotated_image.jpg'.")

def adjust_bboxes_to_pred(only_inside_ocr_bboxes, pred):
    """
    Ajusta as coordenadas das bounding boxes subtraindo o canto superior esquerdo da previsão.


    Args:
    only_inside_ocr_bboxes (list of dicts): Lista de bounding boxes do OCR.
    pred (list): Lista com as coordenadas [x1, y1, x2, y2] da bounding box prevista.

    Returns:
    list of dicts: Lista atualizada com as bounding boxes ajustadas.
    """
    x1_pred, y1_pred = pred[0], pred[1]
    print('y1_pred', y1_pred)
    adjusted_bboxes = []
    for item in only_inside_ocr_bboxes:
        # Coordenadas da bounding box original
        original_bbox = item['boundingBox']
        # Ajusta cada ponto da bounding box
        adjusted_bbox = [
            original_bbox[0] - x1_pred, original_bbox[1] - y1_pred,
            original_bbox[2] - x1_pred, original_bbox[3] - y1_pred,
            original_bbox[4] - x1_pred, original_bbox[5] - y1_pred,
            original_bbox[6] - x1_pred, original_bbox[7] - y1_pred,
        ]
        # Atualiza o dicionário com a bounding box com os valores ajustados para as bounding boxes serem correspondentes com as da imagem recortada
        adjusted_item = item.copy()
        adjusted_item['boundingBox'] = adjusted_bbox
        adjusted_bboxes.append(adjusted_item)

    return adjusted_bboxes

def convert_to_rectangular_format(adjusted_bboxes):
    """
    Converte bounding boxes poligonais para o formato retangular (x1, y1, x2, y2).

    Args:
    adjusted_bboxes (list of dicts): Lista de bounding boxes ajustadas no formato poligonal.

    Returns:
    list of dicts: Lista atualizada com as bounding boxes no formato retangular.
    """
    rectangular_bboxes = []
    for item in adjusted_bboxes:
        # Definem-se os pontos com as coordenadas ajustadas à imagem recortada
        points = item['boundingBox']
        x_coords = points[0::2]  # Todos os valores de x
        y_coords = points[1::2]  # Todos os valores de y

        # Encontra os valores min e max para formar o retângulo
        x1 = min(x_coords) # canto superior esquerdo
        y1 = min(y_coords) # canto superior esquerdo
        x2 = max(x_coords) # canto inferior direito
        y2 = max(y_coords) # canto inferior direito

        # Formato retangular da bounding box que deu entrada, como parâmetro
        rectangular_bbox = [x1, y1, x2, y2]

        # Atualização do dicionário das deteções de OCR com a bounding box convertida para formato retangular
        rectangular_item = item.copy()
        rectangular_item['boundingBox'] = rectangular_bbox
        rectangular_bboxes.append(rectangular_item)

    return rectangular_bboxes

def pil_image_to_bytes(image_pil):
        # Cria um buffer de bytes
        io_buf = io.BytesIO()

        # Guarda a imagem em PIL no buffer de bytes
        image_pil.save(io_buf, format='JPEG')

        # Obtém os bytes do buffer
        return io_buf.getvalue()
# Função que prevê as caixas delimitadoras numa imagem redimensionada com o modelo YOLO treinado para a deteção dos cabeçalhos ou da altura das linhas, que for passado como parâmetro
# recebe uma imagem PIL redimensionada
# Retorna uma lista de tuplos onde cada tuplo contem as coordenadas das caixas delimitadoras e a confiança da previsão.
def predict_bboxes_c_l(image, modelo):
    # Converte a imagem PIL para o formato OpenCV
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    # Faz a previsão com o modelo YOLO
    predictions = modelo(image_cv)
    bboxes = []
    if hasattr(predictions[0].boxes, 'xyxy') and hasattr(predictions[0].boxes, 'conf'):
        xyxy = predictions[0].boxes.xyxy
        confs = predictions[0].boxes.conf
        for i, box in enumerate(xyxy):
            x1, y1, x2, y2 = map(int, [box[0], box[1], box[2], box[3]])
            score = confs[i]
            bboxes.append(((x1, y1, x2, y2), score.item()))
    return bboxes

# método que converte uma imagem em PIL para uma imagem cv2
def pil_to_cv2(imagem_pil):
    # Converter a imagem PIL para um numpy array
    imagem_np = np.array(imagem_pil)

    # Converter a imagem de RGB (PIL) para BGR (OpenCV)
    imagem_cv2 = cv2.cvtColor(imagem_np, cv2.COLOR_RGB2BGR)
    return imagem_cv2

def process_images_with_nms_and_vertical_lines(results, imagem, output_directory_nms):
    Path(output_directory_nms).mkdir(parents=True, exist_ok=True)
    horizontal_apos_nms = []

    if isinstance(imagem, Image.Image):
        image = pil_to_cv2(imagem)
    # image = cv2.imread(str(image_path))
    if image is None:
        print(f"A Imagem não foi encontrada.")

    image_width = image.shape[1]
    image_height = image.shape[0]

    # Assegurar que box_coords seja uma lista de listas ( a 2 dimensões)
    box_coords = [[int(box[0][0]), 0, int(box[2][0]), image_height] for box, _ in results]

    # Verifica se box_coords é não-vazia
    if box_coords:
        box_tensor = tf.convert_to_tensor(box_coords, dtype=tf.float32)
        score_tensor = tf.convert_to_tensor([score for _, score in results], dtype=tf.float32)

        # Aplicar o método NMS
        selected_indices = tf.image.non_max_suppression(
            boxes=box_tensor,
            scores=score_tensor,
            max_output_size=1000,
            iou_threshold=0.000001,
            score_threshold=float('-inf')
        )

        selected_boxes = tf.gather(box_coords, selected_indices).numpy()
        for x1, y1, x2, y2 in selected_boxes:
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)
        horizontal_apos_nms = list(selected_boxes)
        output_image_path = output_directory_nms + '/delimitacao_vertical_da_tabela_apos_nms.jpg'
        print(f"Imagem processada com NMS e guardada em: {output_image_path}")

    return horizontal_apos_nms

def process_images_with_nms_and_horizontal_lines(results, image_cv):
    if isinstance(image_cv, Image.Image):
        image = pil_to_cv2(image_cv)
    # image = cv2.imread(str(image_path))
    if image is None:
        print(f"A Imagem não foi encontrada.")

    horizontal_apos_nms = []

    image_width = image.shape[1]
    probabilities = [score for _, score in results]

    # Assegurar que a box_coords seja uma lista de listas (2-D)
    box_coords = [[0, int(box[0][1]), image_width, int(box[2][1])] for box, _ in results]
    image_duplicada = image
    for x1, y1, x2, y2 in box_coords:
        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)
    cv2.imwrite(str('linhas_horizontais_toda.jpg'), image_duplicada)

    # Verifica se a box_coords é não-vazia e se tem o formato correto
    if box_coords:
        box_tensor = tf.convert_to_tensor(box_coords, dtype=tf.float32)
        score_tensor = tf.convert_to_tensor(probabilities, dtype=tf.float32)

        # Aplicar o NMS com os parâmetros que melhor se adequam para a remoção destas linhas
        selected_indices = tf.image.non_max_suppression(
            boxes=box_tensor,
            scores=score_tensor,
            max_output_size=1000,
            iou_threshold=0.000001,
            score_threshold=float('-inf')
        )

        horizontal_apos_nms = tf.gather(box_coords, selected_indices).numpy()
        for x1, y1, x2, y2 in horizontal_apos_nms:
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 1)
        horizontal_apos_nms = list(horizontal_apos_nms)
        cv2.imwrite(str('linhas_horizontais_com_nms.jpg'), image)
        print(f"Imagem processada com NMS e guardada.")

    return horizontal_apos_nms

## Método que retorna as dimensões das imagens
def get_image_dimensions(image_pil):
    return image_pil.width, image_pil.height

def redimensionar_imagem(imagem):

    # Calcula a razão de aspecto
    largura, altura = imagem.size
    if largura > altura:
        nova_largura = 640
        nova_altura = int((altura * 640) / largura)
    else:
        nova_altura = 640
        nova_largura = int((largura * 640) / altura)

    # Redimensiona a imagem
    imagem_redimensionada = imagem.resize((nova_largura, nova_altura), Image.Resampling.LANCZOS)

    return imagem_redimensionada

def verificar_textos_duplicados(celulas_textos):
    duplicados = {}
    for (row, col), text in celulas_textos.items():
        if row not in duplicados:
            duplicados[row] = {}
        if text not in duplicados[row]:
            duplicados[row][text] = []
        duplicados[row][text].append(col)
    return duplicados

def ajustar_textos_duplicados(celulas_textos):
    duplicados = verificar_textos_duplicados(celulas_textos)
    for row, texts in duplicados.items():
        for text, cols in texts.items():
            if len(cols) > 1:
                # Manter o texto apenas na primeira coluna onde aparece
                primeira_col = min(cols)
                for col in cols:
                    if col != primeira_col:
                        celulas_textos.pop((row, col))
    return celulas_textos




def verificar_textos_duplicados(celulas_textos):
    duplicados = {}
    for (row, col), text in celulas_textos.items():
        if row not in duplicados:
            duplicados[row] = {}
        if text not in duplicados[row]:
            duplicados[row][text] = []
        duplicados[row][text].append(col)
    return duplicados

def ajustar_textos_duplicados(celulas_textos):
    duplicados = verificar_textos_duplicados(celulas_textos)
    for row, texts in duplicados.items():
        for text, cols in texts.items():
            if len(cols) > 1:
                # Manter o texto apenas na primeira coluna onde aparece
                primeira_col = min(cols)
                for col in cols:
                    if col != primeira_col:
                        celulas_textos.pop((row, col))
    return celulas_textos

def intersecao_retangulos(rect1, rect2):
    # rect1 e rect2 são tuplas com as coordenadas (x_min, y_min, x_max, y_max)
    x_min1, y_min1, x_max1, y_max1 = rect1
    x_min2, y_min2, x_max2, y_max2 = rect2

    # Calcula as coordenadas da área de interseção
    x_min_inter = max(x_min1, x_min2)
    y_min_inter = max(y_min1, y_min2)
    x_max_inter = min(x_max1, x_max2)
    y_max_inter = min(y_max1, y_max2)

    # Verifica se existe interseção
    if x_min_inter < x_max_inter and y_min_inter < y_max_inter:
        largura_inter = x_max_inter - x_min_inter
        altura_inter = y_max_inter - y_min_inter
        area_intersecao = largura_inter * altura_inter
    else:
        area_intersecao = 0  # Não existe interseção

    return area_intersecao

# Função para contar o número interseções
def contar_intersecoes_linhas(bbox, bboxes_list):
    intersecoes = 0
    proporcoes = []
    for bbox2 in bboxes_list:
        if intersects_json(bbox, bbox2):
            intersecoes += 1
            proporcoes.append((intersecao_retangulos(bbox, bbox2)/intersecao_retangulos(bbox,bbox)))
    return intersecoes, proporcoes

def remover_ultima_somar_penultima(lista_floats):
    if len(lista_floats) < 2:
        raise ValueError("A lista deve conter pelo menos dois elementos.")

    # Remover o último elemento da lista e guarda o valor
    ultimo_valor = lista_floats.pop()

    # Adiciona o valor removido ao valor que agora é o último da lista
    lista_floats[-1] += ultimo_valor

    return lista_floats

def dividir_string_por_proporcoes(s, proporcoes):
    palavras = s.split(' ')
    total_caracteres = len(s)
    lista_resultante = []
    if len(proporcoes)==1:
      lista_resultante.append(s)
      return lista_resultante

    i = 0
    for idx, proporcao in enumerate(proporcoes):
        # Se for a última iteração, incluir todas as palavras restantes na última string
        if idx == len(proporcoes) - 1:
            string_restante = ' '.join(palavras[i:])
            lista_resultante.append(string_restante)
            break

        limite_caracteres = total_caracteres * proporcao
        string_atual = ''
        caracteres_atual = 0

        while i < len(palavras) and (caracteres_atual + len(palavras[i]) + (1 if caracteres_atual > 0 else 0)) <= limite_caracteres:
            if string_atual:
                string_atual += ' '  # Adicionar espaço entre as palavras
                caracteres_atual += 1  # Contar com o espaço entre as palavras

            string_atual += palavras[i]
            caracteres_atual += len(palavras[i])
            i += 1

        lista_resultante.append(string_atual)

    # Garantir que a lista tenha exatamente o número de strings igual ao numero de celulas com o qual a caixa de texto se interseta
    while len(lista_resultante) < len(proporcoes):
        lista_resultante.append('')

    if len(lista_resultante) > len(proporcoes):
        # Unir todas as strings restantes na última posição para garantir que a lista tenha apenas o número strings correspondente ao n de celulas com que se interseta
        excess_string = ' '.join(lista_resultante[2:])
        lista_resultante = lista_resultante[:2] + [excess_string]
    props_resultantes = []
    for i in range(len(lista_resultante)):
      props_resultantes.append(len(lista_resultante[i])/total_caracteres)
    print(props_resultantes)
    return lista_resultante


# Função principal para processar os dados e gerar o ficheiro estruturado JSON e a tabela formatada representada num ficheiro Excel
def create_excel_and_json(lista_sem_linha_final_amais, ocr_data):

        print(len(lista_sem_linha_final_amais[1]))
        # para evitar que nos casos em que o OCR se interseta com mais que uma bounding box, apareça a mesma string ainda por dividir
        string_anterior = ''
        # ir buscar o nome do ficheiro sem extensão da lista dos resultados de OCR
        # lista onde estarão as coordenadas das células
        bboxes_celulas_todas = []
        # lista onde estarão os ids das células
        id_celula_linha_coluna = []
        # Remover os valores a mais da lista com as coordenadas das células
        x_values = sorted(remover_duplicados(lista_sem_linha_final_amais[0]))
        y_values = sorted(remover_duplicados(lista_sem_linha_final_amais[1]))
        ocr_partido = False
        ocupado = False
        # criar a lista vazia que terá os resultados finais do JSON
        dados_formatados_para_1_fornecedor = []
        # lista que vai conter as bounding boxes das células dos cabeçalhos
        bboxes_cabecalho = []
        coluna_atual = -1
        for j in range(len(y_values)-1):
            for i in range(len(x_values)-1):
                bbox_cells = [x_values[i], y_values[j], x_values[i + 1], y_values[j + 1]]
                bboxes_celulas_todas.append(bbox_cells)
                id_celula_linha_coluna.append([j, i])
                if j == 0:
                    bboxes_cabecalho.append(bbox_cells)

        # criar e preencher a lista que vai conter todas as caixas delimitadoras e os textos detetados pelo OCR, com as coordenadas de 2 cantos
        bboxes_textos_todos = []
        for i in range(len(ocr_data)):
            bbox_fixed = convert_bbox_8_to_4(ocr_data[i][0])
            bboxes_textos_todos.append([bbox_fixed, ocr_data[i][1]])
        # criar o dicionário com as coordenadas das células e o texto que vai aparecer dentro
        celulas_textos = {}
        palavras_adicionadas = set()
        count_headers = 0
        for i in range(len(bboxes_celulas_todas)):
            if id_celula_linha_coluna[i][0] == 0:
                count_headers+=1
        # percorrer todas as bounding boxes com as coordenadas das células das tabelas
        for i in range(len(bboxes_celulas_todas)):
            # se pertencer ao cabeçalho
            if id_celula_linha_coluna[i][0] == 0:  # Cabeçalho
                print(id_celula_linha_coluna[i])
                for j in range(len(bboxes_textos_todos)):
                    # considerar apenas as bounding boxes de OCR for uma lista ou um tuplo e se as bounding boxes de texto se intersetarem com as bboxes de células
                    bboxes_textos_todos_cabec = bboxes_textos_todos[j][0]
                    texto_bboxes_textos_todos_cabec = bboxes_textos_todos[j][1]
                    bboxes_celulas_todas_atual = bboxes_celulas_todas[i]
                    if isinstance(bboxes_textos_todos[j][0], (list, tuple)) and intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j][0]):
                        print('bounding boxes cabecalho OCR', bboxes_textos_todos[j][1])
                        # se uma bounding box de texto se intersetar com mais de uma bbox de células da estutura da tabela
                        if contar_intersecoes(bboxes_textos_todos[j][0], bboxes_cabecalho) > 1:
                            # extrai-se o texto presente nessa caixa de texto
                            string = bboxes_textos_todos[j][1]
                            # se o conteudo da string contiver '|', a string é dividida onde aparecer esse caracter
                            palavras = DivideInfo().run(string)
                            # coluna onde a palavra é colocada
                            col_idx = id_celula_linha_coluna[i][1]

                            # para cada troço de texto, verificar se a palavra já foi adicionada
                            iterador = 0
                            for palavra in palavras:
                                print(palavra)
                                ocupado = False
                                # se o troço ainda não tiver sido adicionado
                                if palavra not in palavras_adicionadas:
                                    bboxes_celulas_todas_coluna_anterior = bboxes_celulas_todas[i]
                                    bboxes_textos_todos_coluna_anterior = bboxes_textos_todos[j-1][0]
                                    # bounding box interseta-se com ocr anterior
                                    interseta_se_com_ocr_anterior = intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0])
                                    # garantir que a proxima coluna não fica vazia é importante no ilovepdf
                                    bbox_seguinte_intersetase_com_cabecalho_seguinte = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0])
                                    count_headers
                                    bbox_seguinte_interseta_se_com_ocr_atual = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0])
                                    # interseta-se com uma coluna anterior?
                                    if intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]) or intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0]):
                                        print('interseta-se com a coluna com id', col_idx)
                                        # A palavra a adicionar vai ser a ultima palavra adicionada e depois junta-se-lhe o resto, porque algumas estão recortadas e não poderia ser simplesmente o OCR anterior, como no caso da imagem ISB_page_4.
                                        if ocr_partido:
                                            word_to_add = celulas_textos[(0, col_idx)]+' '+palavra
                                        elif not intersects(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]):
                                            col_idx += 1
                                            celulas_textos[(0, col_idx)]=palavra
                                            ocupado = True
                                            continue

                                        else:
                                            word_to_add=bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        ocupado = True

                                    # caso em que a bbox de texto se interseta com a celula anterior e com a seguinte
                                    if coluna_atual == -1:
                                        cabec_anterior_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j][0])
                                    elif coluna_atual != -1:
                                        # NB PIECES tem de entrar
                                        # o cabeçalho atual interseta-se com o OCR atual
                                        cabec_anterior_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[coluna_atual], bboxes_textos_todos[j][0]) # não sei se não deveria retirar daqui o -2
                                    cabec_seguinte_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j][0])
                                    if i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual and intersects_json(bboxes_celulas_todas[i-1], bboxes_textos_todos[j][0]) == False:
                                        print('entrou')
                                        word_to_add = bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        col_idx+=1

                                    # a coluna anterior verdadeira interseta-se o OCR atual e o OCR atual interseta-se com a coluna atual
                                    cabec_anterior_verdadeiro_interseta_com_ocr_atual = intersects_json(bboxes_celulas_todas[i-1], bboxes_textos_todos[j][0])
                                    print(bboxes_textos_todos[j][1])
                                    if cabec_anterior_verdadeiro_interseta_com_ocr_atual==True and i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual:
                                        word_to_add = bboxes_textos_todos[j-1][1]+' '+palavra
                                        celulas_textos[(0, col_idx-1)] = word_to_add
                                        col_idx+=1

                                    # para os casos em que vem a seguir de uma coisa com inicio de OCR junto (ver se faz sentido restringir assim ou não)
                                    if i > 0 and i < len(bboxes_celulas_todas)-2 and cabec_anterior_interseta_com_ocr_atual and iterador == 0 and cabec_seguinte_interseta_com_ocr_atual and bbox_seguinte_intersetase_com_cabecalho_seguinte == False:
                                        col_idx += 1
                                        word_to_add = palavra
                                        celulas_textos[(0, col_idx)] = word_to_add
                                        ocupado = True
                                        col_idx+=1

                                    # quando o conteudo de mais do que uma célula do cabeçalho está na primeira caixa de OCR
                                    if i == 0 and cabec_anterior_interseta_com_ocr_atual and cabec_seguinte_interseta_com_ocr_atual and cabec_seguinte_interseta_com_ocr_atual:
                                        print('entrou')
                                        celulas_textos[(0, col_idx)] = palavra
                                        col_idx+=1
                                        if palavra == palavras[-1]:
                                            col_idx-=1


                                    # quando a palavra se interseta com a coluna anterior preciso de garantir que a prox bbox n fica vazia !!!!!!!!!!!!!!!!!!!!!!!
                                    # interfersa_2_page_2 não entra nesta condição
                                    # se a proxima bbox de texto não se interseta com a bbox de célula seguinte
                                    elif i < len(bboxes_celulas_todas)-2 and j < len(bboxes_textos_todos)-2 and not intersects_json(bboxes_celulas_todas[i+1], bboxes_textos_todos[j+1][0]) and intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0 and ocupado == False:
                                        celulas_textos[(0, col_idx+1)] = palavra
                                        ocr_partido = True

                                    # se agora começar a dar mal, tenho de descomentar aqui
                                    # se a caixa de OCR anterior se conectar com a célula em que queremos adicionar o texto
                                    else:
                                        if levou_espaco_e_juntou_com_a_coluna_anterior:
                                            col_idx -=1
                                        if iterador == 0 and levou_espaco_e_juntou_com_a_coluna_anterior == False:
                                            col_idx -= 1
                                        if (0, col_idx) in celulas_textos and ocupado == False and len(celulas_textos):
                                            col_idx += 1
                                            celulas_textos[(0, col_idx)] = palavra
                                            ocr_partido = True
                                            levou_espaco_e_juntou_com_a_coluna_anterior = False
                                        if len(celulas_textos) > count_headers:
                                            celulas_textos[(0, col_idx-1)] = celulas_textos[(0, col_idx-1)]+' '+palavra
                                    # quando se interseta com a coluna anterior
                                    if intersects_json(bboxes_celulas_todas[i], bboxes_textos_todos[j-1][0]) and iterador == 0:
                                        col_idx += 1
                                    palavras_adicionadas.add(palavra)
                                    iterador +=1

                        else: # só tem uma interseção
                            key = (0, id_celula_linha_coluna[i][1])
                            if key not in celulas_textos:
                                celulas_textos[key] = bboxes_textos_todos[j][1]
                                levou_espaco_e_juntou_com_a_coluna_anterior = False
                                coluna_atual = i
                            else:
                                celulas_textos[key] += ' ' + bboxes_textos_todos[j][1]
                                levou_espaco_e_juntou_com_a_coluna_anterior = True
                                coluna_atual = i

            else:  # Outras células
                # criar a lista de bounding boxes de ocr, sem texto
                bboxes_ocr_sem_texto = []
                for j in range(len(bboxes_textos_todos)):
                    bboxes_ocr_sem_texto.append(bboxes_textos_todos[j][0])
                print(bboxes_ocr_sem_texto)

                for j in range(len(bboxes_textos_todos)):
                    if bboxes_textos_todos[j][1] == '1':
                        print(' assim ')
                    if isinstance(bboxes_textos_todos[j][0], (list, tuple)) and intersects_content(bboxes_celulas_todas[i], bboxes_textos_todos[j][0]):
                        key = (id_celula_linha_coluna[i][0], id_celula_linha_coluna[i][1])
                        # se o ocr se intersetar com mais do que uma célula
                        if contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[0] > 1:
                            # vamos extrair o texto presente nessa caixa de texto
                            string = bboxes_textos_todos[j][1]
                            print(string)
                            if string == string_anterior:
                                continue
                            string_anterior = string
                            if string == '12/015924 SU REFERENCIA':
                                print('entrou')

                            print(i+contar_intersecoes(bboxes_textos_todos[j][0], bboxes_celulas_todas)-1)
                            celula_a_direita = bboxes_celulas_todas[i+contar_intersecoes(bboxes_textos_todos[j][0], bboxes_celulas_todas)-1]
                            proxima_ocr = []
                            proxima_ocr.append(bboxes_textos_todos[j+1][0])
                            print(type(proxima_ocr))
                            # verificar se a ultima celula de intersecao do ocr não se interseta com o ocr seguinte
                            if contar_intersecoes(celula_a_direita, proxima_ocr) == 0:
                                palavras_linhas = dividir_string_por_proporcoes(string, contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[1])

                            # verificar se a ultima celula de intersecao com o ocr se interseta com um proximo ocr
                            elif contar_intersecoes(celula_a_direita, proxima_ocr) > 0:
                                print(bboxes_textos_todos[j][0])
                                print(remover_ultima_somar_penultima(contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)[1]))
                                resultado = contar_intersecoes_linhas(bboxes_textos_todos[j][0], bboxes_celulas_todas)
                                proporcoes = remover_ultima_somar_penultima(resultado[1])
                                palavras_linhas = dividir_string_por_proporcoes(string, proporcoes)
                            col_idx = id_celula_linha_coluna[i][1]
                            for palavra_linha in palavras_linhas:
                                print(palavra_linha)
                                key = (id_celula_linha_coluna[i][0], col_idx)
                                if key not in celulas_textos:
                                    celulas_textos[key] = palavra_linha

                                    col_idx +=1
                                elif key in celulas_textos:
                                    print(type(celulas_textos[key]))
                                    celulas_textos[key] += ' ' + palavra_linha

                        else:
                            if key not in celulas_textos:
                                celulas_textos[key] = bboxes_textos_todos[j][1]
                            else:
                                celulas_textos[key] += ' ' + bboxes_textos_todos[j][1]

        celulas_textos_ordenado = {k: celulas_textos[k] for k in sorted(celulas_textos.keys(), key=lambda x: (x[1], x[0]))}
        for key, text in celulas_textos_ordenado.items():
            row, col = key
            if row < len(y_values) - 1 and col < len(x_values) - 1:
                i = row * (len(x_values) - 1) + col
                bbox = bboxes_celulas_todas[i]
                dados_formatados_para_1_fornecedor.append([text, row, col, bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1], [bbox[0], bbox[1]], [bbox[2], bbox[3]]])

        json_data = converter_para_json(dados_formatados_para_1_fornecedor, 'tabela_resultante')
        create_excel_from_json(json_data, 'tabela_resultante')


def somar_canto_sup_esq_da_previsao(resized_lista_coords_x_y, pred):
    # Extrair coordenadas X e Y da lista
    coords_x = resized_lista_coords_x_y[0]
    coords_y = resized_lista_coords_x_y[1]


    # Extrair os valores de pred
    valor_x = pred[0][0]
    valor_y = pred[0][1]

    # Adicionar os valores de pred às coordenadas de X e de Y
    novas_coords_x = [x + valor_x for x in coords_x]
    novas_coords_y = [y + valor_y for y in coords_y]

    # Retornar a lista ajustada
    return [novas_coords_x, novas_coords_y]

def transformar_para_resultados_ocr(only_inside_ocr_bboxes):
    # Criar uma nova lista para armazenar os resultados transformados
    ocr_results = []

    # Iterar sobre cada dicionário na lista original
    for item in only_inside_ocr_bboxes:
        # Extrair o boundingBox e o text do dicionário
        bbox = item['boundingBox']
        text = item['text']
        # Adicionar uma lista [bbox, text] na lista de resultados
        ocr_results.append([bbox, text])

    # Retornar a nova lista transformada
    return ocr_results

# Função para ajustar as coordenadas
def adjust_bounding_boxes_to_table_coords_pred(cropped_text_result, pred):
    x_offset, y_offset = pred[0][0], pred[0][1]

    adjusted_result = []
    for item in cropped_text_result:
        adjusted_bbox = []
        for i in range(0, len(item['boundingBox']), 2):
            x = item['boundingBox'][i] + x_offset
            y = item['boundingBox'][i + 1] + y_offset
            adjusted_bbox.extend([x, y])

        adjusted_item = {
            'text': item['text'],
            'boundingBox': adjusted_bbox,
            'conf': item['conf']
        }

        adjusted_result.append(adjusted_item)

    return adjusted_result

if __name__== "__main__":

    inicio = time.time()
    caminho_imagem = 'data_input/todas_as_imagens_de_fornecedores_orientacao_correta/PROBERTBOSCHLDA_1_page_1.jpg'

    # Abrir e redimensionar a imagem para 2400, porque é recomendado pela Microsoft para bons resultados do OCR
    image_2400 = load_and_resize_image(caminho_imagem, 2400)[0]

    # Converter a imagem redimensionada para bytes
    image_2400_bytes = pil_image_to_bytes(image_2400)

    # Aplicar o Azure OCR
    text_result, angle = azure_api.get_ocr_azure(image_2400_bytes) # Resultados OCR, ângulo de rotação

    # mostrar os resultados do OCR
    image_to_show = image_2400

    # Converter o formato da imagem para PIL e Rodar com o angulo obtido com o OCR
    imagem_rodada = rotate_image(image_2400, angle)[0]

    # Redimensionar a imagem para 640 por 640 para ser sujeita a previsão pela rede neuronal
    imagem = ImageOps.pad(imagem_rodada, (640, 640), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')

    # Obter a imagem já redimensionada para 2400 por 2400 para aplicar OCR no final
    imagem_2400 = ImageOps.pad(imagem_rodada, (2400, 2400), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')

    # modelo treinado com 250 épocas
    model = YOLO('runs_small_dataset_2/weights/best.pt')

    # Previsão da imagem com o modelo treinado
    pred = predict_bboxes(imagem, model)


    if not pred:
        # Se não tiver previsão, o método é interrompido.
        print('Não foi possível obter previsões.')

    else:
        # recortar a imagem, eliminar a parte da imagem que se encontra fora da bounding box
        imagem_recortada = crop(imagem_2400, pred[0])
        imagem_recortada.save('recorta_2400.jpeg', format='JPEG')
        imagem_recortada_bytes = pil_image_to_bytes(imagem_recortada)

        cropped_text_result, cropped_angle = azure_api.get_ocr_azure(imagem_recortada_bytes)# aplicar ocr novamente
        print(cropped_text_result)

        only_inside_ocr_bboxes = adjust_bounding_boxes_to_table_coords_pred(cropped_text_result, pred)

        annotate_image(imagem_2400, only_inside_ocr_bboxes, 'ocr_da_imagem_recortada_convertido_para_imagem_grande.jpg', "red")
        print(' ')
        # criar sem_coisas_de_fora, que é as coordenadas de ocr obtidas + pred[0][0] nas coordenadas de x e +pred[0][1] nas coordenadas de y

        # Guardar a imagem recortada tendo em conta a previsão, para efeitos de verificação
        imagem_recortada.save('output_data/tableDetection/resultado_imagem_recortada_codigo_completo.jpeg')

        print(len(text_result))
        print(len(only_inside_ocr_bboxes))
        print('sem bounding boxes de texto detetado pelo OCR fora da região prevista', only_inside_ocr_bboxes)

        annotate_image_with_inclined_bounding_boxes(imagem_2400, only_inside_ocr_bboxes)

        #
        adjusted_bboxes = adjust_bboxes_to_pred(only_inside_ocr_bboxes, pred[0])

        # anotação dos resultados do OCR na imagem recortada com as coordenadas convertidas
        annotate_image_with_inclined_bounding_boxes(imagem_recortada, adjusted_bboxes)

        converted_adjusted_bboxes = convert_to_rectangular_format(adjusted_bboxes)
        print('coordenadas convertidas', converted_adjusted_bboxes)

        # Conversão da imagem recortada para dimensões 640 por 640 para a próxima etapa de divisão da estrutura da tabela
        # podemos fazer a conversão assim porque a dimensão maior da imagem recortada é sempre superior a 640.
        # converter a imagem recortada para um formato 640 por 640 (resultados melhores na rede neuronal que segue)
        imagem_recortada_640 = redimensionar_imagem(imagem_recortada)
        # imagem_recortada_640 = ImageOps.pad(imagem_recortada, (640, 640), method=Image.Resampling.LANCZOS, centering=(0.5, 0.5), color='white')
        '''Redimensionar a imagem sem as bordas brancas para tornar a conversão das coordenadas imediata, em vez de ser assim'''
        imagem_recortada_640.save('recortadinha.jpeg', format='JPEG')
        # Carregar o modelo treinado para detetar os cabeçalhos das tabelas
        model_c = YOLO('runs_interest_1000_cabecalho_200_epochs/weights/best.pt')

        # Previsão das bounding boxes de cabeçalhos nas imagens 640
        bboxes_scores = predict_bboxes_c_l(imagem_recortada_640, model_c)

        save_annotated_image(imagem_recortada_640, bboxes_scores, 'previsao_cabecalhos_caso_sem_tds_bboxes')

        # Guardar os Resultados da Previsão do modelo treinado para os cabeçalhos
        results = []
        for bbox, score in bboxes_scores:
            x1, y1, x2, y2 = bbox
            # Alteração para o formato que facilita o desenho da linha vertical de uma ponta à outra da imagem, tendo em conta as coordenadas de bboxes previstas
            transformed_bbox = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            results.append((transformed_bbox, score))
        print(results)

        # Lista com coordenadas de y do canto inferior esquerdo de cada bounding box prevista para cabeçalho
        final_results = []

        # Itera sobre os resultados para extrair as coordenadas
        for bbox, _ in results:
            # bbox tem a forma [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            # y do canto inferior esquerdo é o segundo elemento de bbox[3], que é [x1, y2]
            final_results.append(bbox[3][1])
        print(final_results)

        ########### Coordenadas do canto superior esquerdo para ver qual é a mínima e ser o topo da linha dos cabeçalhos
        # Lista para armazenar os resultados finais com o nome do arquivo e as coordenadas de y do canto superior esquerdo de cada bounding box
        final_results_upper_left = []

        for bbox, _ in results:
            # bbox tem a forma [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            # y do canto superior esquerdo é o segundo elemento de bbox[0], que é [x1, y1]
            print(bbox[0][1])
            final_results_upper_left.append(bbox[0][1])

        print(final_results_upper_left)

        ####### Obter o valor de y mínimo para o cabeçalho
        # Itera sobre os resultados finais para calcular a média das coordenadas de y
        if final_results_upper_left:  # Verifica se a lista não está vazia
            # Calcula a média das coordenadas de y
            y_min = min(final_results_upper_left)
            final_results_minimo_cabecalho = int(y_min)  # Converte a média para inteiro para uso na função de desenho
        else:
            # Se não houver coordenadas, podemos definir um valor padrão ou simplesmente omitir
            final_results_minimo_cabecalho = None

        print(final_results_minimo_cabecalho)
        # print(final_results_minimo_cabecalho[1])

        # Lista para armazenar os resultados com a média das coordenadas de y para cada imagem


        if final_results:  # Verifica se a lista não está vazia
            # Calcula a média das coordenadas de y inferior, para definir a parte de baixo do cabeçalho
            final_results_medias = int(np.mean(final_results))
        else:
            # Se não houver coordenadas, podemos definir um valor padrão ou simplesmente omitir
            final_results_medias= None

        print(final_results_medias)

        output_directory_nms = 'output_data_ocr_twice/tableStructure'
        # print(process_images_with_nms_and_vertical_lines(results, input_directory, output_directory_nms))

        vertical_apos_nms = process_images_with_nms_and_vertical_lines(results, imagem_recortada_640, output_directory_nms)
        print('vanms',vertical_apos_nms)

        coordenadas_x = []
        #  x_coords = []
        for bbox in vertical_apos_nms:
            coordenadas_x.extend([bbox[0], bbox[2]])  # Adiciona o y_min e o y_max de cada bounding box
        # coordenadas_x.append(x_coords)

        # Imprime a lista coordenadas_x para a visualização
        print('coordenadas_x',coordenadas_x)

        x_values = sorted(coordenadas_x)  # Primeiro ordenamos os valores para garantir comparações corretas
        coords_x = []
        i = 0
        while i < len(x_values):
            # distancia até 20 pixels para considerar a linha vertical que se segue na divisão das colunas
            if i + 1 < len(x_values) and (x_values[i + 1] - x_values[i] < 20):
                # Se a diferença entre valores consecutivos é menor que 20, remove da lista o menor valor (ou seja, i, já que estão ordenados)
                coords_x.append(x_values[i + 1])  # Adiciona o maior dos dois valores de x
                i += 2  # Avança dois passos para evitar dupla contagem
            else:
                coords_x.append(x_values[i])
                i += 1

        # Certifica-se de que o último valor seja considerado caso não fosse parte de um par comparado
        if len(x_values) > 1 and i == len(x_values) - 1 and abs(x_values[-1] - coords_x[-1]) >= 20:
            coords_x.append(x_values[-1])

        # Imprime a lista coords_y para visualização - Estas são as coordenadas para juntar
        print('coordenadas de x', coords_x)

        # Adiciona 0 no início se o primeiro valor de x for maior que 20
        if coords_x[0] > 20:
            coords_x.insert(0, 0)

        # Adiciona 640 no final se o último valor de x for menor que 620
        if coords_x[-1] < 620:
            coords_x.append(640)

        adjusted_coords_x = coords_x

        # Imprime a lista de coordenadas ajustadas para visualização
        print('Coordenadas ajustadas de x:', adjusted_coords_x)

        # Utiliza 'adjusted_coords_x' para criar os arrays finais e processar as imagens
        # array_x_lista apenas serve para representar as delimitações obtidas no final deste processo
        array_x_lista = []

        for j in range(len(adjusted_coords_x)-1):
            array_x_lista.append(np.array([adjusted_coords_x[j], 0, adjusted_coords_x[j+1], 640]))

        # Impressão final dos resultados para confirmar a estrutura correta dos dados
        print('Lista final de arrays x:', array_x_lista)

        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)
        for box in array_x_lista:
            print(box)
            # Desenha cada bounding box
            start_point = (box[0], box[1])  # Coordenadas do canto superior esquerdo
            end_point = (box[2], box[3])  # Coordenadas do canto inferior direito
            color = (255, 0, 0)  # Cor da bounding box em BGR (azul)
            thickness = 1  # Espessura da linha da bounding box
            cv2.rectangle(image, start_point, end_point, color, thickness)

        # Guarda a imagem anotada na pasta de output
        save_path = os.path.join('output_data_ocr_twice/tableStructure/imagem_recortada_640_linhas_verticais.jpg')
        cv2.imwrite(save_path, image)

        # Carregar o modelo treinado
        model_l = YOLO('runs_interest_1000_altura_linhas_249_epochs/weights/best.pt')

        results_h = []

        bboxes_scores = predict_bboxes_c_l(imagem_recortada_640, model_l)

        save_annotated_image(imagem_recortada_640, bboxes_scores, 'previsao_altura_linhas')

        for bbox, score in bboxes_scores:
            x1, y1, x2, y2 = bbox
            # Alteração para o formato que facilita o desenho de linha vertical de uma pomta à outra da imagem, tendo em conta as coordenadas de bboxes previstas
            transformed_bbox = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]
            results_h.append((transformed_bbox, score))
        print(results_h)



        horizontal_apos_nms = process_images_with_nms_and_horizontal_lines(results_h, imagem_recortada_640)
        print(horizontal_apos_nms)

        coordenadas_y = []

        for bbox in horizontal_apos_nms:
            coordenadas_y.extend([bbox[1], bbox[3]])  # Adiciona y_min e y_max de cada bounding box

        print('coordenadas_y', coordenadas_y)

        '''Parte mais difícil'''

        ####### Experimentar com os limites inferior e superior do cabeçalho ######

        # Tenta abrir a imagem para obter sua altura
        image_height = imagem_recortada_640.height

        # Encontra a média de y para o cabeçalho desta imagem em final_results_medias

        header_y = final_results_medias

        # Se header_y não foi encontrado ou a lista de y_values está vazia, use um valor padrão ou omita
        if header_y is None:
            print(f"Aviso: Coordenada de divisão de cabeçalho não encontrada. Usando valor padrão.")
            header_y = 0  # ou continue para omitir o arquivo

        header_begin = final_results_minimo_cabecalho


        # Se header_y não foi encontrado ou a lista de y_values está vazia, use um valor padrão ou omita
        if header_begin is None:
            print(f"Aviso: Coordenada de divisão de cabeçalho não encontrada. Usando valor padrão.")
            header_begin = 0  # ou continue para omitir o arquivo

        # Adiciona a média de y para o final vertical do cabeçalho e o minimo do inicio vertical do cabeçalho à lista de y_values e ordena
        y_values = sorted(coordenadas_y + [header_y]+[header_begin])

        new_y_values = []
        i = 0
        while i < len(y_values):
            if i + 1 < len(y_values) and (y_values[i + 1] - y_values[i] < 6):
                new_y_values.append(y_values[i + 1])  # Adiciona o maior dos dois valores para a coordenada de y
                i += 2  # Avança dois passos para evitar dupla contagem
            else:
                new_y_values.append(y_values[i])
                i += 1

        # Certifica-se de que o último valor seja considerado se não for parte de um par comparado
        if len(y_values) > 1 and i == len(y_values) - 1 and abs(y_values[-1] - new_y_values[-1]) >= 6:
            new_y_values.append(y_values[-1])

        # Se o último valor de y é menor que a altura da imagem menos 40 pixels, adiciona a altura da imagem
        if new_y_values[-1] < (image_height - 40):
            new_y_values.append(image_height)

        coords_y_div_cabecalho_inteiro = new_y_values
        # coords_y_div_cabecalho_inteiro.append([filename, new_y_values])

        print('coords_y_div_cabecalho_inteiro',coords_y_div_cabecalho_inteiro)
        # Imprime a lista coords_y para visualização
        # print(coords_y_div_cabecalho_inteiro)

        '''Fim da parte mais difícil'''

        array_y_lista_div_cabecalho_inteiro = []
        for j in range(len(coords_y_div_cabecalho_inteiro)-1):
            array_y_lista_div_cabecalho_inteiro.append(np.array([0, coords_y_div_cabecalho_inteiro[j], 640, coords_y_div_cabecalho_inteiro[j+1]]))
        print('array_y_lista',array_y_lista_div_cabecalho_inteiro)


        # Processamento de cada imagem e bounding box

        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)
        if image is not None:
            for box in array_y_lista_div_cabecalho_inteiro:
                # Desenhe cada bounding box
                start_point = (box[0], box[1])  # Coordenadas do canto superior esquerdo
                end_point = (box[2], box[3])  # Coordenadas do canto inferior direito
                color = (255, 0, 0)  # Cor da bounding box (azul)
                thickness = 1  # Espessura da linha da bounding box
                cv2.rectangle(image, start_point, end_point, color, thickness)

            # Guardar a imagem anotada na pasta de destino
            # save_path = os.path.join(destination_folder, item[0])
            # cv2.imwrite('output_data/tableStructure/recortada_anotada_com_linhas.jpg', image)
        else:
            print('A Imagem não foi encontrada.')

        # Criar uma nova lista que combina as coords_x e coords_y
        lista_coords_x_y = [coords_x, coords_y_div_cabecalho_inteiro]
        print(len(coords_y_div_cabecalho_inteiro))
        print('lista_coords_x_y', lista_coords_x_y)

        #...
        if isinstance(imagem_recortada_640, Image.Image):
            image = pil_to_cv2(imagem_recortada_640)

        if image is None:
            print(f"Erro a carregar a imagem.")

        # Desenhar linhas verticais
        for x in lista_coords_x_y[0]:
            cv2.line(image, (x, 0), (x, image.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in lista_coords_x_y[1]:
            cv2.line(image, (0, y), (image.shape[1], y), (0, 255, 0), 2)  # Linhas verdes

        # Salvar a imagem anotada
        # cv2.imwrite('output_data/tableStructure/anotada_linhas_verticais_horizontais_640.jpg', image)
        print(f"Imagem anotada guardada.")

        # imagem_recortada = Image.open('resultado_imagem_recortada_codigo_completo_alterado.jpeg')



        # Criar a lista com as coordenadas redimensionadas
        resized_lista_coords_x_y = []
        width_640, height_640 = get_image_dimensions(imagem_recortada_640)
        width_grandes, height_grandes = get_image_dimensions(imagem_recortada)
        resized_x_values = [x * (width_grandes/width_640) for x in lista_coords_x_y[0]]
        resized_y_values = [y * (height_grandes/height_640) for y in lista_coords_x_y[1]]
        print(len(resized_y_values))
        resized_lista_coords_x_y = [resized_x_values, resized_y_values]
        print(resized_lista_coords_x_y)

        if isinstance(imagem_recortada, Image.Image):
            image = pil_to_cv2(imagem_recortada)

        if image is None:
            print(f"Erro a carregar a imagem.")
        # Desenhar linhas verticais
        for x in resized_lista_coords_x_y[0]:
            cv2.line(image, (int(x), 0), (int(x), image.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(image, (0, int(y)), (image.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes

        print('pred', pred)
        # Guardar a imagem anotada
        cv2.imwrite("linhas_horizontais_verticais_redimensionadas.jpg", image)
        print(f"Imagem anotada guardada")

        # ocr_results para 1 imagem
        # somar a esta lista, nas coordenadas de x o valor de x do canto superior esquerdo da previsão
        # somar a esta lista, nas coordenadas de y o valor de y do canto superior esquerdo da previsão
        resized_lista_coords_x_y = somar_canto_sup_esq_da_previsao(resized_lista_coords_x_y, pred)
        print('resized_lista_x_y', resized_lista_coords_x_y)

        if isinstance(imagem_rodada, Image.Image):
            image_ = pil_to_cv2(imagem_rodada)

        if image_ is None:
            print(f"Erro ao carregar a imagem.")
        # Desenhar linhas verticais
        for x in resized_lista_coords_x_y[0]:
            cv2.line(image_, (int(x), 0), (int(x), image_.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(image_, (0, int(y)), (image_.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes

        print('pred', pred)
        # Salvar a imagem anotada
        cv2.imwrite("translacao_coordenadas_celulas.jpg", image_)
        print(f"Imagem anotada guardada")

        # falta determinar o ocr_results para 1 imagem
        ocr_results = transformar_para_resultados_ocr(only_inside_ocr_bboxes)

        # Exibir os resultados
        print(ocr_results)

        resized_lista_coords_x_y = sorted(resized_lista_coords_x_y, key=lambda x: x[0])
        print(len(resized_lista_coords_x_y))
        print(len(resized_lista_coords_x_y[1]))
        ocr_results = sorted(ocr_results, key=lambda x: x[0])

        new_resized_list_1 = remove_bboxes_without_ocr(ocr_results, resized_lista_coords_x_y, caminho_imagem)
        print(len(new_resized_list_1[1]))
        lista_sem_linha_final_amais_1= remove_unneeded_y_coords(ocr_results, new_resized_list_1)

        print(len(lista_sem_linha_final_amais_1))
        print(len(lista_sem_linha_final_amais_1[1]))
        # create_excel_and_json(lista_sem_linha_final_amais_1, ocr_results)
        create_excel_and_json(resized_lista_coords_x_y, ocr_results)

        caminho_novo = 'ocr_da_imagem_recortada_convertido_para_imagem_grande.jpg'
        imagennnnn = cv2.imread(caminho_novo)

        # Desenhar linhas verticais
        for x in resized_lista_coords_x_y[0]:
            cv2.line(imagennnnn, (int(x), 0), (int(x), imagennnnn.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(imagennnnn, (0, int(y)), (imagennnnn.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes

        print('pred', pred)
        # Salvar a imagem anotada
        cv2.imwrite("translacao_coordenadas_celulas.jpg", imagennnnn)

        if isinstance(imagem_rodada, Image.Image):
            image_ = pil_to_cv2(imagem_rodada)
        for x in lista_sem_linha_final_amais_1[0]:
            cv2.line(imagennnnn, (int(x), 0), (int(x), imagennnnn.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas
            cv2.line(image_, (int(x), 0), (int(x), image_.shape[0]), (255, 0, 0), 2)  # Linhas vermelhas

        # Desenhar linhas horizontais
        for y in resized_lista_coords_x_y[1]:
            cv2.line(image_, (0, int(y)), (image_.shape[1], int(y)), (0, 255, 0), 2)  # Linhas verdes
            cv2.line(imagennnnn, (0, int(y)), (imagennnnn.shape[1], int(y)), (0, 255, 0), 2)

        cv2.imwrite("translacao_coordenadas_celulas_apos_alteracoes.jpg", imagennnnn)

        fim = time.time()
        tempo_execucao=fim-inicio
        # tempo de execução em segundos
        print(f"Tempo de execução: {tempo_execucao:.6f} segundos")